"""
ë°ì´í„°ì…‹ ê´€ë¦¬ ì‹œìŠ¤í…œ
ë‹¤ì–‘í•œ í˜•íƒœì˜ vision ê²€ì‚¬ ë°ì´í„°ì…‹ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import shutil
import tarfile
import zipfile
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import logging
import requests
from tqdm import tqdm
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatasetManager:
    """
    Vision ê²€ì‚¬ìš© ë°ì´í„°ì…‹ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    
    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:
    1. ë‹¤ì–‘í•œ í˜•íƒœì˜ ì••ì¶• íŒŒì¼ í•´ì œ (tar, zip, etc.)
    2. ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ë° ì •ë¦¬
    3. MVTecê³¼ ê°™ì€ í‘œì¤€ ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ
    4. ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë° ì²˜ë¦¬
    5. ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
    """
    
    def __init__(self, data_dir: str = None):
        """
        ë°ì´í„°ì…‹ ë§¤ë‹ˆì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            data_dir: ë°ì´í„°ì…‹ì„ ì €ì¥í•  ê¸°ë³¸ ë””ë ‰í† ë¦¬
        """
        if data_dir is None:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë”ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
            self.data_dir = Path.cwd().parent / "data"
        else:
            self.data_dir = Path(data_dir)
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ê°ì¢… í•˜ìœ„ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.raw_data_dir = self.data_dir / "raw"
        self.processed_data_dir = self.data_dir / "processed"
        self.temp_dir = self.data_dir / "temp"
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤ ìƒì„±
        for dir_path in [self.raw_data_dir, self.processed_data_dir, self.temp_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì´ˆê¸°í™”ë¨: {self.data_dir}")
    
    def extract_archive(self, archive_path: Path, extract_to: Path = None) -> bool:
        """
        ë‹¤ì–‘í•œ í˜•íƒœì˜ ì••ì¶• íŒŒì¼ì„ í•´ì œí•©ë‹ˆë‹¤.
        
        Args:
            archive_path: ì••ì¶• íŒŒì¼ ê²½ë¡œ
            extract_to: ì••ì¶• í•´ì œí•  ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ê²°ì •)
            
        Returns:
            bool: ì••ì¶• í•´ì œ ì„±ê³µ ì—¬ë¶€
        """
        if extract_to is None:
            extract_to = self.temp_dir / archive_path.stem
        
        extract_to.mkdir(parents=True, exist_ok=True)
        
        try:
            logger.info(f"ğŸ“¦ ì••ì¶• íŒŒì¼ í•´ì œ ì‹œì‘: {archive_path}")
            
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì••ì¶• í•´ì œ ë°©ì‹ ê²°ì •
            if archive_path.suffix.lower() in ['.tar', '.tar.gz', '.tgz']:
                with tarfile.open(archive_path, 'r:*') as tar:
                    tar.extractall(path=extract_to)
                    
            elif archive_path.suffix.lower() == '.zip':
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_to)
                    
            else:
                logger.error(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì••ì¶• í˜•ì‹: {archive_path.suffix}")
                return False
            
            logger.info(f"âœ… ì••ì¶• í•´ì œ ì™„ë£Œ: {extract_to}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def normalize_dataset_structure(self, dataset_path: Path, target_name: str) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.
        
        MVTec ë°ì´í„°ì…‹ì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:
        dataset_name/
        â”œâ”€â”€ train/
        â”‚   â””â”€â”€ good/
        â””â”€â”€ test/
            â”œâ”€â”€ good/
            â””â”€â”€ defect_type1/
            â””â”€â”€ defect_type2/
        â””â”€â”€ ground_truth/
            â””â”€â”€ defect_type1/
            â””â”€â”€ defect_type2/
            
        Args:
            dataset_path: ì •ë¦¬í•  ë°ì´í„°ì…‹ ê²½ë¡œ
            target_name: ìµœì¢… ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            Dict: ë°ì´í„°ì…‹ êµ¬ì¡° ì •ë³´
        """
        logger.info(f"ğŸ”§ ë°ì´í„°ì…‹ êµ¬ì¡° ì •ë¦¬ ì‹œì‘: {dataset_path}")
        
        result = {
            "success": False,
            "dataset_path": None,
            "categories": [],
            "train_samples": 0,
            "test_samples": 0,
            "structure": {}
        }
        
        try:
            # ìµœì¢… ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
            final_dataset_path = self.processed_data_dir / target_name
            
            # ê¸°ì¡´ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë°±ì—…
            if final_dataset_path.exists():
                backup_path = final_dataset_path.with_suffix('.backup')
                if backup_path.exists():
                    shutil.rmtree(backup_path)
                shutil.move(final_dataset_path, backup_path)
                logger.info(f"ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ë°±ì—…í–ˆìŠµë‹ˆë‹¤: {backup_path}")
            
            # ë°ì´í„°ì…‹ êµ¬ì¡° íƒìƒ‰ ë° ì •ë¦¬
            self._explore_and_organize_dataset(dataset_path, final_dataset_path, result)
            
            # êµ¬ì¡° ê²€ì¦
            if self._validate_dataset_structure(final_dataset_path, result):
                result["success"] = True
                result["dataset_path"] = str(final_dataset_path)
                logger.info(f"âœ… ë°ì´í„°ì…‹ êµ¬ì¡° ì •ë¦¬ ì™„ë£Œ: {final_dataset_path}")
            else:
                logger.error("âŒ ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ì…‹ êµ¬ì¡° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return result
    
    def _explore_and_organize_dataset(self, source_path: Path, target_path: Path, result: Dict):
        """
        ë°ì´í„°ì…‹ì„ íƒìƒ‰í•˜ê³  í‘œì¤€ êµ¬ì¡°ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸ” ë°ì´í„°ì…‹ êµ¬ì¡° íƒìƒ‰ ì¤‘...")
        
        # ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ë£¨íŠ¸ ì°¾ê¸°
        dataset_root = self._find_dataset_root(source_path)
        
        if dataset_root is None:
            logger.error("âŒ ìœ íš¨í•œ ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ“ ë°ì´í„°ì…‹ ë£¨íŠ¸ ë°œê²¬: {dataset_root}")
        
        # í‘œì¤€ êµ¬ì¡°ë¡œ ë³µì‚¬
        target_path.mkdir(parents=True, exist_ok=True)
        
        # train, test, ground_truth ë””ë ‰í† ë¦¬ ì²˜ë¦¬
        for subset in ['train', 'test', 'ground_truth']:
            source_subset = dataset_root / subset
            target_subset = target_path / subset
            
            if source_subset.exists():
                shutil.copytree(source_subset, target_subset, dirs_exist_ok=True)
                
                # ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
                if subset == 'train':
                    result["train_samples"] = self._count_images(target_subset)
                elif subset == 'test':
                    result["test_samples"] = self._count_images(target_subset)
                
                # ì¹´í…Œê³ ë¦¬ ì •ë³´ ìˆ˜ì§‘
                if subset in ['test', 'ground_truth']:
                    categories = [d.name for d in target_subset.iterdir() if d.is_dir()]
                    result["categories"].extend(categories)
        
        # ì¤‘ë³µ ì œê±°
        result["categories"] = list(set(result["categories"]))
        
        # êµ¬ì¡° ì •ë³´ ê¸°ë¡
        result["structure"] = self._analyze_structure(target_path)
    
    def _find_dataset_root(self, search_path: Path) -> Optional[Path]:
        """
        ì‹¤ì œ ë°ì´í„°ì…‹ì´ ì‹œì‘ë˜ëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        """
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ train, test í´ë”ë¥¼ ì°¾ê¸°
        if (search_path / 'train').exists() and (search_path / 'test').exists():
            return search_path
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸° (ìµœëŒ€ 3ë‹¨ê³„ê¹Œì§€)
        for depth in range(1, 4):
            for item in search_path.rglob('*'):
                if item.is_dir() and item.name in ['train', 'test']:
                    potential_root = item.parent
                    if (potential_root / 'train').exists() and (potential_root / 'test').exists():
                        return potential_root
        
        return None
    
    def _count_images(self, directory: Path) -> int:
        """
        ë””ë ‰í† ë¦¬ ë‚´ì˜ ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        count = 0
        
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in image_extensions:
                count += 1
        
        return count
    
    def _analyze_structure(self, dataset_path: Path) -> Dict[str, Any]:
        """
        ë°ì´í„°ì…‹ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        """
        structure = {}
        
        for subset_dir in dataset_path.iterdir():
            if subset_dir.is_dir():
                subset_name = subset_dir.name
                structure[subset_name] = {}
                
                for category_dir in subset_dir.iterdir():
                    if category_dir.is_dir():
                        category_name = category_dir.name
                        image_count = self._count_images(category_dir)
                        structure[subset_name][category_name] = image_count
        
        return structure
    
    def _validate_dataset_structure(self, dataset_path: Path, result: Dict) -> bool:
        """
        ë°ì´í„°ì…‹ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
        """
        required_dirs = ['train', 'test']
        
        for required_dir in required_dirs:
            dir_path = dataset_path / required_dir
            if not dir_path.exists():
                logger.error(f"âŒ í•„ìˆ˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {required_dir}")
                return False
        
        # train/good ë””ë ‰í† ë¦¬ í™•ì¸
        train_good = dataset_path / 'train' / 'good'
        if not train_good.exists():
            logger.error("âŒ train/good ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ìµœì†Œí•œì˜ ì´ë¯¸ì§€ íŒŒì¼ í™•ì¸
        if result["train_samples"] == 0:
            logger.error("âŒ í›ˆë ¨ìš© ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        if result["test_samples"] == 0:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        logger.info("âœ… ë°ì´í„°ì…‹ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ")
        return True
    
    def process_uploaded_dataset(self, archive_path: str, dataset_name: str) -> Dict[str, Any]:
        """
        ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            archive_path: ì—…ë¡œë“œëœ ì••ì¶• íŒŒì¼ ê²½ë¡œ
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸš€ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘: {archive_path}")
        
        archive_path = Path(archive_path)
        
        # 1ë‹¨ê³„: ì••ì¶• í•´ì œ
        temp_extract_dir = self.temp_dir / f"{dataset_name}_extract"
        if not self.extract_archive(archive_path, temp_extract_dir):
            return {"success": False, "error": "ì••ì¶• í•´ì œ ì‹¤íŒ¨"}
        
        # 2ë‹¨ê³„: êµ¬ì¡° ì •ë¦¬
        result = self.normalize_dataset_structure(temp_extract_dir, dataset_name)
        
        # 3ë‹¨ê³„: ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            shutil.rmtree(temp_extract_dir)
            logger.info("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        return result
    
    def list_available_datasets(self) -> List[Dict[str, Any]]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            List: ë°ì´í„°ì…‹ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        datasets = []
        
        for dataset_dir in self.processed_data_dir.iterdir():
            if dataset_dir.is_dir():
                # ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
                metadata_file = dataset_dir / "dataset_info.json"
                
                dataset_info = {
                    "name": dataset_dir.name,
                    "path": str(dataset_dir),
                    "train_samples": self._count_images(dataset_dir / "train") if (dataset_dir / "train").exists() else 0,
                    "test_samples": self._count_images(dataset_dir / "test") if (dataset_dir / "test").exists() else 0,
                    "structure": self._analyze_structure(dataset_dir)
                }
                
                datasets.append(dataset_info)
        
        return datasets
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict[str, Any]]:
        """
        íŠ¹ì • ë°ì´í„°ì…‹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            
        Returns:
            Dict: ë°ì´í„°ì…‹ ìƒì„¸ ì •ë³´ (ì—†ìœ¼ë©´ None)
        """
        dataset_path = self.processed_data_dir / dataset_name
        
        if not dataset_path.exists():
            return None
        
        return {
            "name": dataset_name,
            "path": str(dataset_path),
            "train_samples": self._count_images(dataset_path / "train") if (dataset_path / "train").exists() else 0,
            "test_samples": self._count_images(dataset_path / "test") if (dataset_path / "test").exists() else 0,
            "structure": self._analyze_structure(dataset_path),
            "categories": [d.name for d in (dataset_path / "test").iterdir() if d.is_dir()] if (dataset_path / "test").exists() else []
        }

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # ë°ì´í„°ì…‹ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    manager = DatasetManager()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ ì¶œë ¥
    datasets = manager.list_available_datasets()
    
    if datasets:
        print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹:")
        for dataset in datasets:
            print(f"  - {dataset['name']}: í›ˆë ¨ {dataset['train_samples']}ê°œ, í…ŒìŠ¤íŠ¸ {dataset['test_samples']}ê°œ")
    else:
        print("ğŸ“‚ ì•„ì§ ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì—¬ ì²˜ë¦¬ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")