"""
PatchCore í•™ìŠµ ê´€ë¦¬ ì‹œìŠ¤í…œ
íŒ€ì›ì˜ Colab í•™ìŠµ ì½”ë“œë¥¼ Python í´ë˜ìŠ¤ë¡œ ë³€í™˜í•œ í•™ìŠµ ê´€ë¦¬ì
"""

import os
import subprocess
import json
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import logging
import threading
from dataclasses import dataclass, asdict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PatchCoreConfig:
    """
    PatchCore í•™ìŠµ ì„¤ì •ì„ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤
    íŒ€ì›ì˜ Colab ì½”ë“œì—ì„œ ì‚¬ìš©ëœ ëª¨ë“  ë§¤ê°œë³€ìˆ˜ë“¤ì„ ì²´ê³„í™”
    """
    # ëª¨ë¸ ê´€ë ¨ ì„¤ì •
    backbone: str = "wideresnet50"  # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬
    layers: List[str] = None  # íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
    pretrain_embed_dimension: int = 1024
    target_embed_dimension: int = 1024
    
    # ì´ìƒ íƒì§€ ê´€ë ¨ ì„¤ì •
    anomaly_scorer_num_nn: int = 7  # k-NNì—ì„œ ì‚¬ìš©í•  ì´ì›ƒ ìˆ˜
    patchsize: int = 3
    
    # ìƒ˜í”Œë§ ê´€ë ¨ ì„¤ì •
    sampling_ratio: float = 0.3  # ìƒ˜í”Œë§ ë¹„ìœ¨ (p ê°’)
    sampler_name: str = "approx_greedy_coreset"
    
    # ë°ì´í„° ê´€ë ¨ ì„¤ì •
    dataset_name: str = "cable"  # ë°ì´í„°ì…‹ ì¹´í…Œê³ ë¦¬
    dataset_type: str = "mvtec"  # ë°ì´í„°ì…‹ íƒ€ì…
    resize_size: int = 320
    image_size: int = 256
    
    # í•™ìŠµ ê´€ë ¨ ì„¤ì •
    seed: int = 0
    gpu_id: int = 0 if None else None  # GPU ì‚¬ìš© ì„¤ì •
    save_model: bool = True
    
    # ì‹¤í—˜ ê´€ë¦¬
    log_group: str = None  # ìë™ ìƒì„±ë¨
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if self.layers is None:
            self.layers = ["layer2", "layer3"]
        
        if self.log_group is None:
            timestamp = datetime.now().strftime("%m%d_%H%M")
            self.log_group = f"MLOps_Vision_{self.backbone}_k{self.anomaly_scorer_num_nn}_p{int(self.sampling_ratio*100)}_{timestamp}"

class PatchCoreTrainer:
    """
    PatchCore ëª¨ë¸ í•™ìŠµì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
    Colabì˜ bash ëª…ë ¹ì–´ë¥¼ Pythonìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë” ìœ ì—°í•œ ì œì–´ë¥¼ ì œê³µ
    """
    
    def __init__(self, patchcore_dir: str = None, results_dir: str = None):
        """
        í•™ìŠµ ê´€ë¦¬ìë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            patchcore_dir: PatchCore ì„¤ì¹˜ ë””ë ‰í† ë¦¬
            results_dir: í•™ìŠµ ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        # PatchCore ë””ë ‰í† ë¦¬ ì„¤ì •
        if patchcore_dir is None:
            self.patchcore_dir = Path.cwd().parent / "patchcore-inspection"
        else:
            self.patchcore_dir = Path(patchcore_dir)
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì„¤ì •
        if results_dir is None:
            self.results_dir = Path.cwd().parent / "models"
        else:
            self.results_dir = Path(results_dir)
        
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
        self.run_script = self.patchcore_dir / "bin" / "run_patchcore.py"
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ í•™ìŠµ ì •ë³´
        self.current_training = None
        self.training_thread = None
        
        logger.info(f"PatchCore í•™ìŠµ ê´€ë¦¬ì ì´ˆê¸°í™”ë¨")
        logger.info(f"  - PatchCore ë””ë ‰í† ë¦¬: {self.patchcore_dir}")
        logger.info(f"  - ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: {self.results_dir}")
    
    def validate_setup(self) -> Dict[str, bool]:
        """
        í•™ìŠµì„ ìœ„í•œ í™˜ê²½ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: ê²€ì¦ ê²°ê³¼
        """
        validation = {
            "patchcore_directory_exists": self.patchcore_dir.exists(),
            "run_script_exists": self.run_script.exists(),
            "results_directory_writable": True,
            "python_environment_ready": True
        }
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ì“°ê¸° ê¶Œí•œ í…ŒìŠ¤íŠ¸
        try:
            test_file = self.results_dir / "test_write.tmp"
            test_file.touch()
            test_file.unlink()
        except Exception:
            validation["results_directory_writable"] = False
        
        # Python í™˜ê²½ í…ŒìŠ¤íŠ¸
        try:
            import sys
            sys.path.append(str(self.patchcore_dir / "src"))
            import patchcore
            logger.info("âœ… PatchCore ëª¨ë“ˆ import ì„±ê³µ")
        except ImportError:
            validation["python_environment_ready"] = False
            logger.error("âŒ PatchCore ëª¨ë“ˆ import ì‹¤íŒ¨")
        
        # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
        all_valid = all(validation.values())
        if all_valid:
            logger.info("âœ… í•™ìŠµ í™˜ê²½ ê²€ì¦ ì™„ë£Œ")
        else:
            logger.error("âŒ í•™ìŠµ í™˜ê²½ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤:")
            for key, value in validation.items():
                if not value:
                    logger.error(f"  - {key}: ì‹¤íŒ¨")
        
        return validation
    
    def start_transfer_learning(self, config: PatchCoreConfig, dataset_path: Path, base_model_path: str, blocking: bool = True) -> Dict[str, Any]:
        """
        ì „ì´í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.
        ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ ë°ì´í„°ì— ëŒ€í•´ ì¶”ê°€ í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        try:
            if self.is_training_active():
                return {"success": False, "error": "ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"}
            
            logger.info(f"ğŸ”„ ì „ì´í•™ìŠµ ì‹œì‘: {base_model_path} -> {config.dataset_name}")
            
            # ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ í™•ì¸
            base_model_full_path = self.results_dir / base_model_path / "weights" / "lightning" / "model.ckpt"
            if not base_model_full_path.exists():
                return {"success": False, "error": f"ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_model_path}"}
            
            # í•™ìŠµ ì •ë³´ ì„¤ì •
            training_info = {
                "type": "transfer_learning",
                "base_model": base_model_path,
                "dataset": config.dataset_name,
                "start_time": datetime.now().isoformat(),
                "status": "starting",
                "progress": 0
            }
            
            self.current_training = training_info
            
            if blocking:
                result = self._run_transfer_learning(config, dataset_path, base_model_full_path)
                self.current_training = None
                return result
            else:
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
                thread = threading.Thread(
                    target=self._run_transfer_learning_background,
                    args=(config, dataset_path, base_model_full_path)
                )
                thread.start()
                return {"success": True, "training_info": training_info}
                
        except Exception as e:
            logger.error(f"ì „ì´í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.current_training = None
            return {"success": False, "error": str(e)}

    def start_retraining(self, config: PatchCoreConfig, dataset_path: Path, base_model_path: str, blocking: bool = True) -> Dict[str, Any]:
        """
        ì¬í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.
        ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„°ë¥¼ ëª¨ë‘ í•©ì³ì„œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµí•©ë‹ˆë‹¤.
        """
        try:
            if self.is_training_active():
                return {"success": False, "error": "ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"}
            
            logger.info(f"ğŸ” ì¬í•™ìŠµ ì‹œì‘: ê¸°ì¡´ ëª¨ë¸ {base_model_path} ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ ì¬í•™ìŠµ")
            
            # ê¸°ì¡´ ëª¨ë¸ì˜ ë°ì´í„°ì…‹ ê²½ë¡œ í™•ì¸
            base_model_dir = self.results_dir / base_model_path
            if not base_model_dir.exists():
                return {"success": False, "error": f"ê¸°ì¡´ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_model_path}"}
            
            # í•™ìŠµ ì •ë³´ ì„¤ì •
            training_info = {
                "type": "retraining",
                "base_model": base_model_path,
                "dataset": config.dataset_name,
                "start_time": datetime.now().isoformat(),
                "status": "starting",
                "progress": 0
            }
            
            self.current_training = training_info
            
            if blocking:
                result = self._run_retraining(config, dataset_path, base_model_dir)
                self.current_training = None
                return result
            else:
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
                thread = threading.Thread(
                    target=self._run_retraining_background,
                    args=(config, dataset_path, base_model_dir)
                )
                thread.start()
                return {"success": True, "training_info": training_info}
                
        except Exception as e:
            logger.error(f"ì¬í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.current_training = None
            return {"success": False, "error": str(e)}

    def _run_transfer_learning(self, config: PatchCoreConfig, dataset_path: Path, base_model_path: Path) -> Dict[str, Any]:
        """ì „ì´í•™ìŠµ ì‹¤í–‰ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            logger.info("ğŸ”„ ì „ì´í•™ìŠµ ì‹¤í–‰ ì¤‘...")
            
            # anomalib ëª¨ë¸ ë¡œë“œ
            from anomalib.models import Patchcore
            from anomalib.engine import Engine
            from anomalib.data import Folder
            
            # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
            model = Patchcore.load_from_checkpoint(str(base_model_path))
            
            # ìƒˆ ë°ì´í„°ì…‹ ì„¤ì •
            datamodule = Folder(
                root=str(dataset_path),
                normal_dir="good",
                abnormal_dir="defect",
                train_batch_size=32,
                eval_batch_size=32,
                image_size=(config.image_size, config.image_size),
            )
            
            # ì „ì´í•™ìŠµìš© ì—”ì§„ ì„¤ì •
            engine = Engine(
                task="segmentation",
                model=model,
                datamodule=datamodule,
                max_epochs=10,  # ì „ì´í•™ìŠµì€ ì ì€ ì—í­ìœ¼ë¡œ
                resume_from_checkpoint=str(base_model_path)  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
            )
            
            # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            result_dir = self.results_dir / f"{config.log_group}_transfer"
            result_dir.mkdir(parents=True, exist_ok=True)
            
            # í•™ìŠµ ì‹¤í–‰
            logger.info("ì „ì´í•™ìŠµ ì§„í–‰ ì¤‘...")
            if self.current_training:
                self.current_training["status"] = "training"
                self.current_training["progress"] = 50
            
            engine.fit()
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            logger.info("ì „ì´í•™ìŠµ í…ŒìŠ¤íŠ¸ ì¤‘...")
            if self.current_training:
                self.current_training["status"] = "testing"
                self.current_training["progress"] = 90
            
            test_results = engine.test()
            
            # ê²°ê³¼ ì €ì¥
            model_save_path = result_dir / "weights" / "lightning"
            model_save_path.mkdir(parents=True, exist_ok=True)
            engine.trainer.save_checkpoint(model_save_path / "model.ckpt")
            
            if self.current_training:
                self.current_training["status"] = "completed"
                self.current_training["progress"] = 100
            
            logger.info("âœ… ì „ì´í•™ìŠµ ì™„ë£Œ")
            return {
                "success": True,
                "model_path": str(result_dir),
                "test_results": test_results
            }
            
        except Exception as e:
            logger.error(f"ì „ì´í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            if self.current_training:
                self.current_training["status"] = "failed"
                self.current_training["error"] = str(e)
            return {"success": False, "error": str(e)}

    def _run_retraining(self, config: PatchCoreConfig, new_dataset_path: Path, base_model_dir: Path) -> Dict[str, Any]:
        """ì¬í•™ìŠµ ì‹¤í–‰ (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            logger.info("ğŸ” ì¬í•™ìŠµ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ì¡´ ë°ì´í„°ì…‹ê³¼ ìƒˆ ë°ì´í„°ì…‹ ë³‘í•©
            merged_dataset_path = self._merge_datasets(base_model_dir, new_dataset_path)
            
            # ì¼ë°˜ í•™ìŠµê³¼ ë™ì¼í•˜ì§€ë§Œ ë³‘í•©ëœ ë°ì´í„°ì…‹ ì‚¬ìš©
            return self._run_training_with_dataset(config, merged_dataset_path, is_retraining=True)
            
        except Exception as e:
            logger.error(f"ì¬í•™ìŠµ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            if self.current_training:
                self.current_training["status"] = "failed"
                self.current_training["error"] = str(e)
            return {"success": False, "error": str(e)}

    def _merge_datasets(self, base_model_dir: Path, new_dataset_path: Path) -> Path:
        """ê¸°ì¡´ ë°ì´í„°ì…‹ê³¼ ìƒˆ ë°ì´í„°ì…‹ì„ ë³‘í•©"""
        import shutil
        
        # ë³‘í•©ëœ ë°ì´í„°ì…‹ ì €ì¥í•  ì„ì‹œ ë””ë ‰í† ë¦¬
        merged_path = self.results_dir / "temp_merged_dataset"
        if merged_path.exists():
            shutil.rmtree(merged_path)
        merged_path.mkdir(parents=True)
        
        # good, defect í´ë” ìƒì„±
        (merged_path / "good").mkdir()
        (merged_path / "defect").mkdir()
        
        # ê¸°ì¡´ ë°ì´í„°ì…‹ ë³µì‚¬ (ê¸°ì¡´ ëª¨ë¸ ë””ë ‰í† ë¦¬ì—ì„œ ì›ë³¸ ë°ì´í„° ê²½ë¡œ ì°¾ê¸°)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ëª¨ë¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì›ë³¸ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì•„ì•¼ í•¨
        
        # ìƒˆ ë°ì´í„°ì…‹ ë³µì‚¬
        if (new_dataset_path / "good").exists():
            for img_file in (new_dataset_path / "good").glob("*"):
                shutil.copy2(img_file, merged_path / "good")
        
        if (new_dataset_path / "defect").exists():
            for img_file in (new_dataset_path / "defect").glob("*"):
                shutil.copy2(img_file, merged_path / "defect")
        
        logger.info(f"ë°ì´í„°ì…‹ ë³‘í•© ì™„ë£Œ: {merged_path}")
        return merged_path

    def _run_transfer_learning_background(self, config: PatchCoreConfig, dataset_path: Path, base_model_path: Path):
        """ë°±ê·¸ë¼ìš´ë“œ ì „ì´í•™ìŠµ ì‹¤í–‰"""
        try:
            result = self._run_transfer_learning(config, dataset_path, base_model_path)
            logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì „ì´í•™ìŠµ ì™„ë£Œ: {result}")
        except Exception as e:
            logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì „ì´í•™ìŠµ ì‹¤íŒ¨: {e}")
        finally:
            self.current_training = None

    def _run_retraining_background(self, config: PatchCoreConfig, dataset_path: Path, base_model_dir: Path):
        """ë°±ê·¸ë¼ìš´ë“œ ì¬í•™ìŠµ ì‹¤í–‰"""
        try:
            result = self._run_retraining(config, dataset_path, base_model_dir)
            logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì¬í•™ìŠµ ì™„ë£Œ: {result}")
        except Exception as e:
            logger.error(f"ë°±ê·¸ë¼ìš´ë“œ ì¬í•™ìŠµ ì‹¤íŒ¨: {e}")
        finally:
            self.current_training = None
    
    def build_command(self, config: PatchCoreConfig, dataset_path: str) -> List[str]:
        """
        PatchCore í•™ìŠµ ëª…ë ¹ì–´ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        
        ì´ í•¨ìˆ˜ëŠ” íŒ€ì›ì˜ Colab ì½”ë“œì—ì„œ ì‚¬ìš©ëœ ë³µì¡í•œ bash ëª…ë ¹ì–´ë¥¼
        Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            config: í•™ìŠµ ì„¤ì •
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ
            
        Returns:
            List[str]: ì‹¤í–‰í•  ëª…ë ¹ì–´ ë¦¬ìŠ¤íŠ¸
        """
        cmd = [
            "python", str(self.run_script),
            "--seed", str(config.seed),
            "--log_group", config.log_group,
            str(self.results_dir),
            "patch_core"
        ]
        
        # GPU ì„¤ì •
        if config.gpu_id is not None:
            cmd.extend(["--gpu", str(config.gpu_id)])
        
        # ëª¨ë¸ ì €ì¥ ì„¤ì •
        if config.save_model:
            cmd.append("--save_patchcore_model")
        
        # ë°±ë³¸ ë„¤íŠ¸ì›Œí¬ ì„¤ì •
        cmd.extend(["-b", config.backbone])
        
        # ë ˆì´ì–´ ì„¤ì •
        for layer in config.layers:
            cmd.extend(["-le", layer])
        
        # ì„ë² ë”© ì°¨ì› ì„¤ì •
        cmd.extend([
            "--pretrain_embed_dimension", str(config.pretrain_embed_dimension),
            "--target_embed_dimension", str(config.target_embed_dimension)
        ])
        
        # ì´ìƒ íƒì§€ ì„¤ì •
        cmd.extend([
            "--anomaly_scorer_num_nn", str(config.anomaly_scorer_num_nn),
            "--patchsize", str(config.patchsize)
        ])
        
        # ìƒ˜í”ŒëŸ¬ ì„¤ì •
        cmd.extend([
            "sampler", 
            "-p", str(config.sampling_ratio),
            config.sampler_name
        ])
        
        # ë°ì´í„°ì…‹ ì„¤ì •
        cmd.extend([
            "dataset",
            "--resize", str(config.resize_size),
            "--imagesize", str(config.image_size),
            "-d", config.dataset_name,
            config.dataset_type,
            str(dataset_path)
        ])
        
        logger.info(f"í•™ìŠµ ëª…ë ¹ì–´ êµ¬ì„± ì™„ë£Œ: {len(cmd)}ê°œ ì¸ì")
        return cmd
    
    def start_training(self, config: PatchCoreConfig, dataset_path: str, blocking: bool = False) -> Dict[str, Any]:
        """
        PatchCore ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.
        
        Args:
            config: í•™ìŠµ ì„¤ì •
            dataset_path: ë°ì´í„°ì…‹ ê²½ë¡œ
            blocking: Trueë©´ í•™ìŠµ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°, Falseë©´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            
        Returns:
            Dict: í•™ìŠµ ì‹œì‘ ê²°ê³¼
        """
        # í™˜ê²½ ê²€ì¦
        validation = self.validate_setup()
        if not all(validation.values()):
            return {
                "success": False,
                "error": "í™˜ê²½ ê²€ì¦ ì‹¤íŒ¨",
                "validation": validation
            }
        
        # ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if self.is_training_active():
            return {
                "success": False,
                "error": "ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤"
            }
        
        try:
            # ëª…ë ¹ì–´ êµ¬ì„±
            cmd = self.build_command(config, dataset_path)
            
            # í•™ìŠµ ì •ë³´ ê¸°ë¡
            training_info = {
                "config": asdict(config),
                "dataset_path": dataset_path,
                "start_time": datetime.now().isoformat(),
                "status": "running",
                "log_group": config.log_group,
                "results_path": str(self.results_dir / config.log_group)
            }
            
            self.current_training = training_info
            
            # ë¡œê·¸ íŒŒì¼ ì„¤ì •
            log_file_path = self.results_dir / f"{config.log_group}_training.log"
            
            logger.info(f"ğŸš€ PatchCore í•™ìŠµ ì‹œì‘: {config.log_group}")
            logger.info(f"ğŸ“Š ì„¤ì •: {config.backbone}, k={config.anomaly_scorer_num_nn}, p={config.sampling_ratio}")
            logger.info(f"ğŸ“ ë°ì´í„°ì…‹: {dataset_path}")
            logger.info(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file_path}")
            
            if blocking:
                # ë™ê¸°ì‹ ì‹¤í–‰ (í•™ìŠµ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°)
                result = self._run_training_sync(cmd, log_file_path)
            else:
                # ë¹„ë™ê¸°ì‹ ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
                result = self._run_training_async(cmd, log_file_path)
            
            return {
                "success": True,
                "training_info": training_info,
                "execution_result": result
            }
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
            self.current_training = None
            return {
                "success": False,
                "error": str(e)
            }
    
    def _run_training_sync(self, cmd: List[str], log_file_path: Path) -> Dict[str, Any]:
        """
        ë™ê¸°ì‹ìœ¼ë¡œ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        start_time = time.time()
        
        try:
            with open(log_file_path, 'w') as log_file:
                # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ PatchCore ë””ë ‰í† ë¦¬ë¡œ ë³€ê²½
                process = subprocess.Popen(
                    cmd,
                    cwd=self.patchcore_dir,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    text=True,
                    env=self._get_training_environment()
                )
                
                # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                return_code = process.wait()
                
                end_time = time.time()
                duration = end_time - start_time
                
                if return_code == 0:
                    logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {duration:.1f}ì´ˆ)")
                    self.current_training["status"] = "completed"
                    self.current_training["end_time"] = datetime.now().isoformat()
                    self.current_training["duration"] = duration
                    
                    return {
                        "success": True,
                        "return_code": return_code,
                        "duration": duration
                    }
                else:
                    logger.error(f"âŒ í•™ìŠµ ì‹¤íŒ¨ (ì¢…ë£Œ ì½”ë“œ: {return_code})")
                    self.current_training["status"] = "failed"
                    self.current_training["error_code"] = return_code
                    
                    return {
                        "success": False,
                        "return_code": return_code,
                        "duration": duration
                    }
                    
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            if self.current_training:
                self.current_training["status"] = "error"
                self.current_training["error"] = str(e)
            
            return {
                "success": False,
                "error": str(e)
            }
    
    def _run_training_async(self, cmd: List[str], log_file_path: Path) -> Dict[str, Any]:
        """
        ë¹„ë™ê¸°ì‹ìœ¼ë¡œ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        def training_worker():
            """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë  í•™ìŠµ ì‘ì—…"""
            try:
                result = self._run_training_sync(cmd, log_file_path)
                logger.info(f"ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì™„ë£Œ: {result}")
            except Exception as e:
                logger.error(f"ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ ì˜¤ë¥˜: {e}")
                if self.current_training:
                    self.current_training["status"] = "error"
                    self.current_training["error"] = str(e)
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        self.training_thread = threading.Thread(target=training_worker, daemon=True)
        self.training_thread.start()
        
        return {
            "success": True,
            "mode": "async",
            "thread_id": self.training_thread.ident
        }
    
    def _get_training_environment(self) -> Dict[str, str]:
        """
        í•™ìŠµì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        """
        env = os.environ.copy()
        
        # PYTHONPATHì— PatchCore src ë””ë ‰í† ë¦¬ ì¶”ê°€
        patchcore_src = str(self.patchcore_dir / "src")
        if "PYTHONPATH" in env:
            env["PYTHONPATH"] = f"{patchcore_src}{os.pathsep}{env['PYTHONPATH']}"
        else:
            env["PYTHONPATH"] = patchcore_src
        
        return env
    
    def is_training_active(self) -> bool:
        """
        í˜„ì¬ í•™ìŠµì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        
        Returns:
            bool: í•™ìŠµ ì§„í–‰ ìƒíƒœ
        """
        if self.current_training is None:
            return False
        
        if self.current_training.get("status") in ["completed", "failed", "error"]:
            return False
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œê°€ ì‚´ì•„ìˆëŠ”ì§€ í™•ì¸
        if self.training_thread and self.training_thread.is_alive():
            return True
        
        return False
    
    def get_training_status(self) -> Optional[Dict[str, Any]]:
        """
        í˜„ì¬ í•™ìŠµ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Returns:
            Dict: í•™ìŠµ ìƒíƒœ ì •ë³´ (í•™ìŠµ ì¤‘ì´ ì•„ë‹ˆë©´ None)
        """
        if self.current_training is None:
            return None
        
        status = self.current_training.copy()
        status["is_active"] = self.is_training_active()
        
        return status
    
    def stop_training(self) -> bool:
        """
        ì§„í–‰ ì¤‘ì¸ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ì¤‘ë‹¨ ì„±ê³µ ì—¬ë¶€
        """
        if not self.is_training_active():
            logger.warning("ì¤‘ë‹¨í•  í•™ìŠµì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # ì—¬ê¸°ì— í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            # í˜„ì¬ëŠ” ê¸°ë³¸ì ì¸ ìƒíƒœ ë³€ê²½ë§Œ ìˆ˜í–‰
            if self.current_training:
                self.current_training["status"] = "stopped"
                self.current_training["stop_time"] = datetime.now().isoformat()
            
            logger.info("ğŸ›‘ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            logger.error(f"âŒ í•™ìŠµ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
            return False

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # í•™ìŠµ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    trainer = PatchCoreTrainer()
    
    # í™˜ê²½ ê²€ì¦
    validation = trainer.validate_setup()
    print(f"í™˜ê²½ ê²€ì¦ ê²°ê³¼: {validation}")
    
    if all(validation.values()):
        print("âœ… í•™ìŠµ í™˜ê²½ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì„¤ì • ìƒì„±
        config = PatchCoreConfig(
            dataset_name="cable",
            anomaly_scorer_num_nn=7,
            sampling_ratio=0.3
        )
        
        print(f"í•™ìŠµ ì„¤ì •: {config.log_group}")
        print("ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì œê³µí•˜ì„¸ìš”.")
    else:
        print("âŒ í•™ìŠµ í™˜ê²½ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")