"""
MLOps Vision í†µí•© ì›¹ API
ëª¨ë“  PatchCore ê´€ë ¨ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©í•˜ëŠ” ë©”ì¸ API
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Dict, Any, List, Optional
from fastapi import Request
from fastapi.responses import RedirectResponse
import json
import logging
from pathlib import Path
import tempfile
import shutil
from datetime import datetime
import threading
import threading
import sqlite3
import uuid
import pickle
import shutil
from training_manager import PatchCoreTrainer, PatchCoreConfig
from fastapi import File, UploadFile, Form
from fastapi.responses import FileResponse
from typing import List

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆë“¤ import
from patchcore_manager import PatchCoreManager
from dataset_manager import DatasetManager
from training_manager import PatchCoreTrainer, PatchCoreConfig
from model_manager import PatchCoreModelManager

#static íŒŒì¼ ì§€ì› ì¶”ê°€
from fastapi.templating import Jinja2Templates

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MLOps Vision Platform",
    description="Vision ê²€ì‚¬ MLOps í†µí•© í”Œë«í¼",
    version="1.0.0"
)

# Static íŒŒì¼ê³¼ í…œí”Œë¦¿ ì„¤ì •
app.mount("/static", StaticFiles(directory="../static"), name="static")
templates = Jinja2Templates(directory="../templates")

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ë“¤
patchcore_manager = PatchCoreManager()
dataset_manager = DatasetManager()
trainer = PatchCoreTrainer()
model_manager = PatchCoreModelManager()
trainer = PatchCoreTrainer()
BASE_DATA_PATH = Path(__file__).resolve().parent / "data" / "processed"

def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    conn = sqlite3.connect('mlops_vision.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS projects (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL UNIQUE,
            description TEXT,
            status TEXT DEFAULT 'active',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ë°ì´í„°ì…‹ í…Œì´ë¸” (í”„ë¡œì íŠ¸ ì—°ê²°)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS project_datasets (
            id TEXT PRIMARY KEY,
            project_id TEXT NOT NULL,
            name TEXT NOT NULL,
            path TEXT NOT NULL,
            image_count INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES projects (id)
        )
    ''')
    
    # ëª¨ë¸ í…Œì´ë¸” (í”„ë¡œì íŠ¸ ì—°ê²°)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS project_models (
            id TEXT PRIMARY KEY,
            project_id TEXT NOT NULL,
            dataset_id TEXT NOT NULL,
            name TEXT NOT NULL,
            backbone TEXT,
            performance_data TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES projects (id),
            FOREIGN KEY (dataset_id) REFERENCES project_datasets (id)
        )
    ''')
    
    conn.commit()
    conn.close()

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ DB ì´ˆê¸°í™”
init_database()

# Pydantic ëª¨ë¸ ì •ì˜ (API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ)
class TrainingConfigModel(BaseModel):
    """í•™ìŠµ ì„¤ì •ì„ ìœ„í•œ Pydantic ëª¨ë¸"""
    backbone: str = "wideresnet50"
    layers: List[str] = ["layer2", "layer3"]
    anomaly_scorer_num_nn: int = 7
    sampling_ratio: float = 0.3
    dataset_name: str  # í•„ìˆ˜ í•„ë“œë¡œ ë³€ê²½
    resize_size: int = 320
    image_size: int = 256
    gpu_id: Optional[int] = None
    
    # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
    training_type: str = "new"  # "new", "transfer", "retrain"
    base_model: Optional[str] = None  # ì „ì´í•™ìŠµ/ì¬í•™ìŠµ ì‹œ ê¸°ì¡´ ëª¨ë¸ëª…
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 10
    train_ratio: float = 0.7
    val_ratio: float = 0.2

class SystemStatusResponse(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    patchcore_installed: bool
    datasets_available: int
    models_trained: int
    training_active: bool
    system_ready: bool

# =============================================================================
# ì‹œìŠ¤í…œ ìƒíƒœ ë° ì´ˆê¸°í™” API
# =============================================================================


@app.get("/", response_class=HTMLResponse)
async def redirect_to_dashboard():
    """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëŒ€ì‹œë³´ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    return RedirectResponse(url="/dashboard")

@app.get("/dashboard", response_class=HTMLResponse)
async def get_dashboard(request: Request):
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€"""
    return templates.TemplateResponse("dashboard.html", {"request": request})

@app.get("/ui/datasets", response_class=HTMLResponse)
async def get_datasets_page(request: Request):
    """ë°ì´í„°ì…‹ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("datasets.html", {"request": request})

@app.get("/ui/training", response_class=HTMLResponse)
async def get_training_page(request: Request):
    """í•™ìŠµ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("training.html", {"request": request})

@app.get("/ui/models", response_class=HTMLResponse)
async def get_models_page(request: Request):
    """ëª¨ë¸ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("models.html", {"request": request})

@app.get("/ui/projects", response_class=HTMLResponse)
async def get_projects_page(request: Request):
    """í”„ë¡œì íŠ¸ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("projects.html", {"request": request})

@app.get("/ui/monitoring", response_class=HTMLResponse)
async def get_monitoring_page(request: Request):
    """ëª¨ë‹ˆí„°ë§ í˜ì´ì§€"""
    return templates.TemplateResponse("monitoring.html", {"request": request})

@app.get("/system/status", response_model=SystemStatusResponse)
async def get_system_status():
    """ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        # PatchCore ì„¤ì¹˜ ìƒíƒœ í™•ì¸
        patchcore_installed = patchcore_manager.is_patchcore_installed()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìˆ˜ í™•ì¸
        datasets = dataset_manager.list_available_datasets()
        datasets_count = len(datasets)
        
        # í•™ìŠµëœ ëª¨ë¸ ìˆ˜ í™•ì¸
        models = model_manager.discover_trained_models()
        models_count = len(models)
        
        # í˜„ì¬ í•™ìŠµ ì§„í–‰ ìƒíƒœ í™•ì¸
        training_active = trainer.is_training_active()
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ
        system_ready = patchcore_installed and datasets_count > 0
        
        return SystemStatusResponse(
            patchcore_installed=patchcore_installed,
            datasets_available=datasets_count,
            models_trained=models_count,
            training_active=training_active,
            system_ready=system_ready
        )
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/system/setup")
async def setup_system():
    """ì „ì²´ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (ì´ë¯¸ ì„¤ì¹˜ëœ êµ¬ì„±ìš”ì†ŒëŠ” ê±´ë„ˆëœë‹ˆë‹¤)"""
    try:
        logger.info("ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...")
        
        setup_results = {
            "patchcore_status": "unknown",
            "actions_taken": [],
            "skipped_actions": [],
            "warnings": []
        }
        
        # 1. PatchCore ì„¤ì¹˜ ìƒíƒœ í™•ì¸
        if patchcore_manager.is_patchcore_installed():
            setup_results["patchcore_status"] = "already_installed"
            setup_results["skipped_actions"].append("PatchCore ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì´ë¯¸ ì„¤ì¹˜ë¨)")
            logger.info("âœ… PatchCoreê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            
            # ì¶”ê°€ ê²€ì¦ë§Œ ìˆ˜í–‰
            verification = patchcore_manager.verify_installation()
            if not verification["installation_complete"]:
                setup_results["warnings"].append("PatchCore ì„¤ì¹˜ê°€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                setup_results["warnings"].extend(verification["error_messages"])
        else:
            logger.info("ğŸ“¦ PatchCore ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤...")
            setup_result = patchcore_manager.setup_patchcore_complete()
            
            if setup_result:
                setup_results["patchcore_status"] = "newly_installed"
                setup_results["actions_taken"].append("PatchCore ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ")
            else:
                setup_results["patchcore_status"] = "installation_failed"
                setup_results["warnings"].append("PatchCore ì„¤ì¹˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # 2. ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸ ë° ìƒì„±
        required_dirs = [
            dataset_manager.data_dir,
            dataset_manager.processed_data_dir,
            trainer.results_dir,
            model_manager.models_archive_dir
        ]
        
        dirs_created = []
        for dir_path in required_dirs:
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                dirs_created.append(str(dir_path.name))
        
        if dirs_created:
            setup_results["actions_taken"].append(f"ë””ë ‰í† ë¦¬ ìƒì„±: {', '.join(dirs_created)}")
        else:
            setup_results["skipped_actions"].append("ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ (ì´ë¯¸ ì¡´ì¬)")
        
        # 3. ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸
        system_ready = setup_results["patchcore_status"] in ["already_installed", "newly_installed"]
        
        # 4. ê²°ê³¼ ì •ë¦¬
        total_actions = len(setup_results["actions_taken"])
        total_skipped = len(setup_results["skipped_actions"])
        
        if system_ready:
            logger.info("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
            return {
                "success": True,
                "system_ready": True,
                "message": f"ì‹œìŠ¤í…œì´ ì‚¬ìš© ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ({total_actions}ê°œ ì‘ì—… ìˆ˜í–‰, {total_skipped}ê°œ ì‘ì—… ê±´ë„ˆëœ€)",
                "details": setup_results,
                "next_steps": [
                    "ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (/datasets/upload)",
                    "í•™ìŠµ ì„¤ì •ì„ êµ¬ì„±í•˜ì„¸ìš” (/training/start)", 
                    "ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”"
                ]
            }
        else:
            logger.error("âŒ ì‹œìŠ¤í…œ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤")
            return {
                "success": False,
                "system_ready": False,
                "message": "ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "details": setup_results,
                "troubleshooting": [
                    "ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ë¥¼ íŒŒì•…í•˜ì„¸ìš”",
                    "í•„ìš”í•œ ê²½ìš° ìˆ˜ë™ìœ¼ë¡œ PatchCoreë¥¼ ì„¤ì¹˜í•´ë³´ì„¸ìš”",
                    "ì‹œìŠ¤í…œ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”"
                ]
            }
            
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "success": False,
            "system_ready": False,
            "error": str(e),
            "message": "ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
    
@app.post("/system/setup/force-reinstall")
async def force_reinstall_system():
    """ê°•ì œë¡œ ì „ì²´ ì‹œìŠ¤í…œì„ ì¬ì„¤ì¹˜í•©ë‹ˆë‹¤. (ì£¼ì˜: ê¸°ì¡´ ì„¤ì¹˜ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤)"""
    try:
        logger.warning("âš ï¸ ê°•ì œ ì¬ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ê°•ì œ ì¬ì„¤ì¹˜ ìˆ˜í–‰
        setup_result = patchcore_manager.setup_patchcore_complete(force_reinstall=True)
        
        if setup_result:
            logger.info("âœ… ê°•ì œ ì¬ì„¤ì¹˜ ì™„ë£Œ")
            return {
                "success": True,
                "message": "ì‹œìŠ¤í…œì´ ê°•ì œë¡œ ì¬ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "warning": "ëª¨ë“  ê¸°ì¡´ ì„¤ì •ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
        else:
            raise HTTPException(status_code=500, detail="ê°•ì œ ì¬ì„¤ì¹˜ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"ê°•ì œ ì¬ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# í”„ë¡œì íŠ¸ ê´€ë¦¬ API
# =============================================================================

@app.get("/api/projects")
async def get_projects():
    """ëª¨ë“  í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ"""
    try:
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT p.id, p.name, p.description, p.status,
                p.created_at, p.updated_at,
                COUNT(DISTINCT d.id) as dataset_count,
                COUNT(DISTINCT m.id) as model_count
            FROM projects p
            LEFT JOIN project_datasets d ON p.id = d.project_id
            LEFT JOIN project_models m ON p.id = m.project_id
            GROUP BY p.id
        ''')
        
        rows = cursor.fetchall()
        projects = []

        for row in rows:
            projects.append({
            "id": row[0],
            "name": row[1],
            "description": row[2],
            "status": row[3],
            "created_at": row[4],
            "updated_at": row[5],
            "dataset_count": row[6],
            "model_count": row[7]
            })

        conn.close()
        return projects # ë°°ì—´ë§Œ return
        
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/projects")
async def create_project(project_data: dict):
    """ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±"""
    try:
        project_name = project_data.get("name")
        project_description = project_data.get("description", "")
        project_status = project_data.get('status', 'active')
        
        if not project_name:
            raise HTTPException(status_code=400, detail="í”„ë¡œì íŠ¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        project_id = str(uuid.uuid4())
        
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO projects (id, name, description, status, created_at, updated_at)
            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        """, (project_id, project_name, project_description, project_status))
        
        conn.commit()
        
        # ìƒì„±ëœ í”„ë¡œì íŠ¸ ì •ë³´ ë°˜í™˜
        cursor.execute('SELECT * FROM projects WHERE id = ?', (project_id,))
        row = cursor.fetchone()
        
        project = {
            "id": row[0],
            "name": row[1],
            "description": row[2],
            "created_at": row[3],
            "updated_at": row[4],
            "dataset_count": 0,
            "model_count": 0
        }
        
        conn.close()
        return {"success": True, "project": project}
        
    except sqlite3.IntegrityError:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„ì…ë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.put("/api/projects/{project_id}")
async def update_project(project_id: str, project_data: dict):
    """í”„ë¡œì íŠ¸ ì •ë³´ ìˆ˜ì •"""
    try:
        project_name = project_data.get("name")
        project_description = project_data.get("description", "")
        project_status = project_data.get('status', 'active')
        
        if not project_name:
            raise HTTPException(status_code=400, detail="í”„ë¡œì íŠ¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE projects
            SET name = ?, description = ?, status = ?, updated_at = CURRENT_TIMESTAMP
            WHERE id = ?
        ''', (project_name, project_description, project_status, project_id))
        
        if cursor.rowcount == 0:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        conn.commit()
        
        # ìˆ˜ì •ëœ í”„ë¡œì íŠ¸ ì •ë³´ ë°˜í™˜
        cursor.execute('SELECT * FROM projects WHERE id = ?', (project_id,))
        row = cursor.fetchone()
        
        project = {
            "id": row[0],
            "name": row[1],
            "description": row[2],
            "status": row[3],
            "created_at": row[4],
            "updated_at": row[5]
        }
        
        conn.close()
        return {"success": True, "project": project}
        
    except sqlite3.IntegrityError:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„ì…ë‹ˆë‹¤")
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ìˆ˜ì • ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/projects/{project_id}")
async def delete_project(project_id: str):
    """í”„ë¡œì íŠ¸ ì‚­ì œ"""
    try:
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()
        
        # í”„ë¡œì íŠ¸ ì¡´ì¬ í™•ì¸
        cursor.execute('SELECT name FROM projects WHERE id = ?', (project_id,))
        project = cursor.fetchone()
        
        if not project:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê´€ë ¨ ë°ì´í„° ì‚­ì œ (ëª¨ë¸ -> ë°ì´í„°ì…‹ -> í”„ë¡œì íŠ¸ ìˆœ)
        cursor.execute('DELETE FROM project_models WHERE project_id = ?', (project_id,))
        cursor.execute('DELETE FROM project_datasets WHERE project_id = ?', (project_id,))
        cursor.execute('DELETE FROM projects WHERE id = ?', (project_id,))
        
        conn.commit()
        conn.close()
        
        return {"success": True, "message": f"í”„ë¡œì íŠ¸ '{project[0]}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
        
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.get("/api/projects/{project_id}/export")
async def export_project(project_id: str):
    """
    í”„ë¡œì íŠ¸ + ë°ì´í„°ì…‹ + ëª¨ë¸ ë‚´ë³´ë‚´ê¸°
    (JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)
    """
    try:
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()

        # í”„ë¡œì íŠ¸ ê°€ì ¸ì˜¤ê¸°
        cursor.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        project_row = cursor.fetchone()
        if not project_row:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        project = {
            "id": project_row[0],
            "name": project_row[1],
            "description": project_row[2],
            "status": project_row[3],
            "created_at": project_row[4],
            "updated_at": project_row[5],
        }

        # ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°
        cursor.execute("SELECT * FROM project_datasets WHERE project_id=?", (project_id,))
        dataset_rows = cursor.fetchall()
        datasets = [dict(zip([col[0] for col in cursor.description], row)) for row in dataset_rows]

        # ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
        cursor.execute("SELECT * FROM project_models WHERE project_id=?", (project_id,))
        model_rows = cursor.fetchall()
        models = [dict(zip([col[0] for col in cursor.description], row)) for row in model_rows]

        conn.close()

        return {
            "project": project,
            "datasets": datasets,
            "models": models
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/projects/import")
async def import_project(file: UploadFile = File(...)):
    """
    í”„ë¡œì íŠ¸ + ë°ì´í„°ì…‹ + ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ZIP íŒŒì¼ ì—…ë¡œë“œ)
    """
    import uuid, os, zipfile, shutil, json
    try:
        # 1. ì—…ë¡œë“œëœ zip ì €ì¥
        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)
        zip_path = os.path.join(upload_dir, file.filename)
        with open(zip_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # 2. ì••ì¶• í•´ì œ
        extract_dir = os.path.join(upload_dir, str(uuid.uuid4()))
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)

        # 3. JSON ë¡œë“œ
        with open(os.path.join(extract_dir, "project.json"), "r", encoding="utf-8") as f:
            import_data = json.load(f)

        project = import_data.get("project")
        datasets = import_data.get("datasets", [])
        models = import_data.get("models", [])

        if not project:
            raise HTTPException(status_code=400, detail="ì˜ëª»ëœ í”„ë¡œì íŠ¸ ë°ì´í„°")

        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()

        # 4. ìƒˆ í”„ë¡œì íŠ¸ ID ìƒì„±
        new_project_id = str(uuid.uuid4())
        cursor.execute("""
            INSERT INTO projects (id, name, description, status, created_at, updated_at)
            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        """, (
            new_project_id,
            project.get("name") + " (ê°€ì ¸ì˜¤ê¸°)",
            project.get("description"),
            project.get("status", "active")
        ))

        dataset_id_map = {}
        # 5. ë°ì´í„°ì…‹ ë³µì‚¬
        for ds in datasets:
            new_dataset_id = str(uuid.uuid4())
            dataset_id_map[ds["id"]] = new_dataset_id

            src_path = os.path.join(extract_dir, "datasets", ds["id"])
            dst_path = os.path.join("processed", new_project_id, new_dataset_id)
            os.makedirs(os.path.dirname(dst_path), exist_ok=True)
            if os.path.exists(src_path):
                shutil.copytree(src_path, dst_path)

            cursor.execute("""
                INSERT INTO project_datasets (id, project_id, name, path, image_count, created_at)
                VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (
                new_dataset_id,
                new_project_id,
                ds.get("name"),
                dst_path,
                ds.get("image_count", 0)
            ))

        # 6. ëª¨ë¸ ë³µì‚¬
        for m in models:
            new_model_id = str(uuid.uuid4())
            new_dataset_id = dataset_id_map.get(m["dataset_id"], None)

            src_model_path = os.path.join(extract_dir, "models", m["id"])
            dst_model_path = os.path.join("saved_models", new_model_id)
            if os.path.exists(src_model_path):
                shutil.copytree(src_model_path, dst_model_path)

            cursor.execute("""
                INSERT INTO project_models (id, project_id, dataset_id, name, backbone, performance_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (
                new_model_id,
                new_project_id,
                new_dataset_id,
                m.get("name"),
                m.get("backbone"),
                m.get("performance_data")
            ))

        conn.commit()
        conn.close()

        return {"success": True, "new_project_id": new_project_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/projects/{project_id}/duplicate")
async def duplicate_project(project_id: str):
    """
    í”„ë¡œì íŠ¸ + ë°ì´í„°ì…‹ + ëª¨ë¸ ì™„ì „ ë³µì œ
    """
    try:
        import uuid, shutil, os

        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()

        # 1. ì›ë³¸ í”„ë¡œì íŠ¸ ì¡°íšŒ
        cursor.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        project_row = cursor.fetchone()
        if not project_row:
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        new_project_id = str(uuid.uuid4())
        new_project_name = project_row[1] + " (ë³µì‚¬ë³¸)"

        # 2. í”„ë¡œì íŠ¸ ë³µì‚¬
        cursor.execute("""
            INSERT INTO projects (id, name, description, status, created_at, updated_at)
            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        """, (new_project_id, new_project_name, project_row[2], project_row[3]))

        # 3. ë°ì´í„°ì…‹ ë³µì‚¬
        cursor.execute("SELECT * FROM project_datasets WHERE project_id=?", (project_id,))
        dataset_rows = cursor.fetchall()
        dataset_id_map = {}

        for ds in dataset_rows:
            new_dataset_id = str(uuid.uuid4())
            dataset_id_map[ds[0]] = new_dataset_id

            new_path = ds[3].replace(project_id, new_project_id)
            cursor.execute("""
                INSERT INTO project_datasets (id, project_id, name, path, image_count, created_at)
                VALUES (?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (new_dataset_id, new_project_id, ds[2], new_path, ds[4]))

            if os.path.exists(ds[3]):
                shutil.copytree(ds[3], new_path)

        # 4. ëª¨ë¸ ë³µì‚¬
        cursor.execute("SELECT * FROM project_models WHERE project_id=?", (project_id,))
        model_rows = cursor.fetchall()

        for m in model_rows:
            new_model_id = str(uuid.uuid4())
            new_dataset_id = dataset_id_map.get(m[2], m[2])

            cursor.execute("""
                INSERT INTO project_models (id, project_id, dataset_id, name, backbone, performance_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
            """, (new_model_id, new_project_id, new_dataset_id, m[3], m[4], m[5]))

            src_model_path = os.path.join("saved_models", m[0])
            dst_model_path = os.path.join("saved_models", new_model_id)
            if os.path.exists(src_model_path):
                shutil.copytree(src_model_path, dst_model_path)

        conn.commit()
        conn.close()

        return {"success": True, "new_project_id": new_project_id, "new_name": new_project_name}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/projects/{project_id}")
async def get_project_detail(project_id: str):
    """í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        conn = sqlite3.connect('mlops_vision.db')
        cursor = conn.cursor()
        
        # í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
        cursor.execute('SELECT * FROM projects WHERE id = ?', (project_id,))
        project_row = cursor.fetchone()
        
        if not project_row:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„°ì…‹ ëª©ë¡
        cursor.execute('SELECT * FROM project_datasets WHERE project_id = ?', (project_id,))
        dataset_rows = cursor.fetchall()
        
        datasets = []
        for row in dataset_rows:
            datasets.append({
                "id": row[0],
                "name": row[1],
                "description": row[2],
                "status": row[3],
                "created_at": row[4],
                "updated_at": row[5]
            })
        
        # ëª¨ë¸ ëª©ë¡
        cursor.execute('SELECT * FROM project_models WHERE project_id = ?', (project_id,))
        model_rows = cursor.fetchall()
        
        models = []
        for row in model_rows:
            models.append({
                "id": row[0],
                "dataset_id": row[2],
                "name": row[3],
                "backbone": row[4],
                "performance_data": json.loads(row[5]) if row[5] else {},
                "created_at": row[6]
            })
        
        project = {
            "id": project_row[0],
            "name": project_row[1],
            "description": project_row[2],
            "created_at": project_row[3],
            "updated_at": project_row[4],
            "datasets": datasets,
            "models": models
        }
        
        conn.close()
        return {"success": True, "project": project}
        
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ë°ì´í„°ì…‹ ê´€ë¦¬ API
# =============================================================================

@app.get("/datasets")
async def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        datasets = dataset_manager.list_available_datasets()
        return {
            "datasets": datasets,
            "total_count": len(datasets)
        }
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/datasets/upload")
async def upload_dataset(
    file: UploadFile = File(...),
    dataset_name: str = Form(...)
):
    """ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"ğŸ“¤ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # ë°ì´í„°ì…‹ ì²˜ë¦¬
            result = dataset_manager.process_uploaded_dataset(tmp_file_path, dataset_name)
            
            return {
                "success": result["success"],
                "dataset_name": dataset_name,
                "processing_result": result
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            Path(tmp_file_path).unlink(missing_ok=True)
            
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/datasets/{dataset_name}")
async def get_dataset_info(dataset_name: str):
    """íŠ¹ì • ë°ì´í„°ì…‹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        dataset_info = dataset_manager.get_dataset_info(dataset_name)
        
        if dataset_info is None:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
        
        return dataset_info
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 
    
@app.post("/datasets/{project_id}/{dataset_id}/upload")
async def upload_dataset_images(
    project_id: str,
    dataset_id: str,
    files: List[UploadFile] = File(...)
):
    try:
        # ê²½ë¡œë¥¼ BASE_DATA_PATH ê¸°ì¤€ìœ¼ë¡œ í†µì¼
        dataset_path = BASE_DATA_PATH / project_id / dataset_id
        dataset_path.mkdir(parents=True, exist_ok=True)

        saved_filenames = []
        for file in files:
            file_path = dataset_path / file.filename
            with open(file_path, "wb") as f:
                shutil.copyfileobj(file.file, f)
            saved_filenames.append(file.filename)

        return {"success": True, "saved_files": saved_filenames}

    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

# ì´ë¯¸ì§€ ì¡°íšŒ APIëŠ” ë™ì¼í•˜ê²Œ ì¶”ê°€
@app.get("/api/image/{project_id}/{dataset_id}/{filename}")
async def get_image(project_id: str, dataset_id: str, filename: str):
    try:
        file_path = BASE_DATA_PATH / project_id / dataset_id / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ StreamingResponseë¡œ ì „ì†¡
        return FileResponse(str(file_path))

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
    
@app.get("/datasets/{project_id}/{dataset_id}/images")
async def list_uploaded_images(project_id: str, dataset_id: str):
    try:
        dataset_path = BASE_DATA_PATH / project_id / dataset_id
        
        if not dataset_path.exists():
            raise HTTPException(status_code=404, detail="ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        images = [f.name for f in dataset_path.iterdir() if f.is_file()]
        return {"images": images}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# í•™ìŠµ ê´€ë¦¬ API
# =============================================================================

@app.get("/training/status")
async def get_training_status():
    """í˜„ì¬ í•™ìŠµ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        status = trainer.get_training_status()
        
        return {
            "is_training_active": trainer.is_training_active(),
            "current_training": status
        }
        
    except Exception as e:
        logger.error(f"í•™ìŠµ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/training/start")
async def start_training(config: TrainingConfigModel):
    """ë°ì´í„°ì…‹ íƒ€ì…ë³„ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤"""
    try:
        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸
        dataset_info = dataset_manager.get_dataset_info(config.dataset_name)
        if dataset_info is None:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.dataset_name}")
        
        # ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if trainer.is_training_active():
            raise HTTPException(status_code=400, detail="ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
        
        # ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ í•™ìŠµ ë¶„ê¸°
        dataset_type = dataset_info.get("type", "anomaly")  # ê¸°ë³¸ê°’ anomaly
        
        if dataset_type == "anomal":  # Anomaly Detection
            # PatchCore ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
            patchcore_config = PatchCoreConfig(
                backbone=config.backbone,
                layers=config.layers,
                anomaly_scorer_num_nn=config.anomaly_scorer_num_nn,
                sampling_ratio=config.sampling_ratio,
                dataset_name=config.dataset_name,
                resize_size=config.resize_size,
                image_size=config.image_size,
                gpu_id=config.gpu_id
            )
            
            logger.info(f"Anomaly Detection í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            result = trainer.start_training(
                config=patchcore_config,
                dataset_path=dataset_info["path"],
                blocking=False
            )
            
        elif dataset_type == "classify":  # Classification
            logger.info(f"Classification í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Classification í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            return {"success": False, "error": "Classification ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        elif dataset_type == "obd":  # Object Detection
            logger.info(f"Object Detection í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Object Detection í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)  
            return {"success": False, "error": "Object Detection ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        elif dataset_type == "seg":  # Segmentation
            logger.info(f"Segmentation í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Segmentation í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            return {"success": False, "error": "Segmentation ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {dataset_type}")
        
        if result["success"]:
            return {
                "success": True,
                "message": f"{dataset_type} ëª¨ë¸ í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
                "training_info": result["training_info"],
                "dataset_type": dataset_type
            }
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨"))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/training/stop")
async def stop_training():
    """ì§„í–‰ ì¤‘ì¸ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."""
    try:
        if not trainer.is_training_active():
            raise HTTPException(status_code=400, detail="ì§„í–‰ ì¤‘ì¸ í•™ìŠµì´ ì—†ìŠµë‹ˆë‹¤")
        
        success = trainer.stop_training()
        
        if success:
            return {"success": True, "message": "í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"}
        else:
            raise HTTPException(status_code=500, detail="í•™ìŠµ ì¤‘ë‹¨ ì‹¤íŒ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í•™ìŠµ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
# í•™ìŠµ ì§„í–‰ë¥  API ì¶”ê°€
@app.get("/training/progress")
async def get_training_progress():
    """ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ë¥  ì¡°íšŒ"""
    try:
        status = trainer.get_training_status()
        progress = 0
        
        if status and status.get("log_group"):
            progress = parse_training_log(status["log_group"])
            
        return {
            "success": True,
            "status": status,
            "progress": progress,
            "is_active": trainer.is_training_active()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# í•™ìŠµ ë¡œê·¸ API ì¶”ê°€
@app.get("/training/logs/{log_group}")
async def get_training_logs(log_group: str, lines: int = 50):
    """í•™ìŠµ ë¡œê·¸ ì¡°íšŒ"""
    try:
        log_file = trainer.results_dir / f"{log_group}_training.log"
        if not log_file.exists():
            raise HTTPException(status_code=404, detail="ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "logs": recent_lines,
            "total_lines": len(all_lines)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def parse_training_log(log_group: str) -> int:
    """ë¡œê·¸ íŒŒì¼ì—ì„œ ì§„í–‰ë¥  ì¶”ì¶œ"""
    try:
        log_file = trainer.results_dir / f"{log_group}_training.log"
        if not log_file.exists():
            return 0
            
        with open(log_file, 'r') as f:
            content = f.read()
            
        # PatchCore ë¡œê·¸ì—ì„œ ì§„í–‰ë¥  íŒŒì‹±
        import re
        
        # "Epoch 3/10" íŒ¨í„´ ì°¾ê¸°
        epoch_pattern = r'Epoch (\d+)/(\d+)'
        matches = re.findall(epoch_pattern, content)
        
        if matches:
            current, total = map(int, matches[-1])
            return min(int((current / total) * 100), 100)
            
        # "Training progress: 45%" ê°™ì€ íŒ¨í„´ë„ ì°¾ê¸°
        progress_pattern = r'progress:\s*(\d+)%'
        progress_matches = re.findall(progress_pattern, content, re.IGNORECASE)
        
        if progress_matches:
            return min(int(progress_matches[-1]), 100)
        
        return 0
    except Exception:
        return 0

# =============================================================================
# ëª¨ë¸ ê´€ë¦¬ API
# =============================================================================

@app.get("/models")
async def list_models():
    """í•™ìŠµëœ ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤ (ê¸°ì¡´ í•™ìŠµ + ì—…ë¡œë“œëœ ëª¨ë¸ í¬í•¨)"""
    try:
        # ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸
        trained_models = model_manager.discover_trained_models()
        
        # ì—…ë¡œë“œëœ ëª¨ë¸ ì¶”ê°€
        uploaded_models = []
        saved_models_dir = Path("saved_models")
        
        if saved_models_dir.exists():
            for model_dir in saved_models_dir.iterdir():
                if model_dir.is_dir():
                    model_info_file = model_dir / "model_info.json"
                    if model_info_file.exists():
                        try:
                            with open(model_info_file, 'r', encoding='utf-8') as f:
                                model_info = json.load(f)
                            uploaded_models.append(model_info)
                        except Exception as e:
                            logger.error(f"ì—…ë¡œë“œ ëª¨ë¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ëª¨ë“  ëª¨ë¸ í•©ì¹˜ê¸°
        all_models = trained_models + uploaded_models
        
        # ëª¨ë¸ ë¹„êµ ë°ì´í„° ìƒì„±
        comparison_data = []
        if all_models:
            # ê¸°ì¡´ comparison ë¡œì§ + ì—…ë¡œë“œëœ ëª¨ë¸ë„ í¬í•¨
            for model in uploaded_models:
                comparison_data.append({
                    "model_name": model["model_name"],
                    "backbone": model["config"].get("backbone", "unknown"),
                    "accuracy": model["performance"].get("accuracy", 0),
                    "f1_score": model["performance"].get("f1_score", 0),
                    "precision": model["performance"].get("precision", 0),
                    "recall": model["performance"].get("recall", 0),
                    "source": "uploaded"
                })
        
        return {
            "models": all_models,
            "total_count": len(all_models),
            "comparison": comparison_data
        }
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/models/{model_name}/archive")
async def create_model_archive(model_name: str):
    """ëª¨ë¸ì„ ì••ì¶• ì•„ì¹´ì´ë¸Œë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë¸ ì°¾ê¸°
        models = model_manager.discover_trained_models()
        target_model = None
        
        for model in models:
            if model["model_name"] == model_name:
                target_model = model
                break
        
        if target_model is None:
            raise HTTPException(status_code=404, detail=f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_name}")
        
        # ì•„ì¹´ì´ë¸Œ ìƒì„±
        result = model_manager.create_model_archive(target_model)
        
        if result["success"]:
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "ì•„ì¹´ì´ë¸Œ ìƒì„± ì‹¤íŒ¨"))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì•„ì¹´ì´ë¸Œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/best")
async def get_best_model(metric: str = "image_auroc"):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        models = model_manager.discover_trained_models()
        
        if not models:
            raise HTTPException(status_code=404, detail="í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        best_model = model_manager.get_best_model(models, metric)
        
        if best_model is None:
            raise HTTPException(status_code=404, detail=f"{metric} ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "best_model": best_model,
            "metric": metric,
            "score": best_model["metadata"]["performance"].get(metric, None)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìµœê³  ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/models/upload")
async def upload_model(
    file: UploadFile = File(...),
    project_id: Optional[str] = Form(None)
):
    """ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    try:
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_ext = file.filename.split('.')[-1].lower()
        if file_ext not in ['pkl', 'pth']:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
        
        # ëª¨ë¸ ID ìƒì„±
        model_id = f"model_{int(datetime.now().timestamp())}"
        model_name = file.filename.rsplit('.', 1)[0]
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        saved_models_dir = Path("saved_models")
        saved_models_dir.mkdir(exist_ok=True)
        
        model_dir = saved_models_dir / f"{model_name}_{model_id}"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        model_path = model_dir / file.filename
        with open(model_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # pkl íŒŒì¼ ë¶„ì„
        model_info = analyze_uploaded_model(str(model_path), model_id, model_name, project_id)
        
        # model_info.json ì €ì¥
        with open(model_dir / "model_info.json", 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: {model_path}")
        return {"success": True, "model": model_info}
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.delete("/models/{model_id}")
async def delete_model(model_id: str):
    """ì—…ë¡œë“œëœ ëª¨ë¸ ì‚­ì œ"""
    try:
        saved_models_dir = Path("saved_models")
        
        if not saved_models_dir.exists():
            raise HTTPException(status_code=404, detail="ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        for model_dir in saved_models_dir.iterdir():
            if model_dir.is_dir():
                model_info_file = model_dir / "model_info.json"
                
                if model_info_file.exists():
                    try:
                        with open(model_info_file, 'r', encoding='utf-8') as f:
                            model_info = json.load(f)
                        
                        if model_info.get('id') == model_id:
                            shutil.rmtree(model_dir)
                            logger.info(f"ëª¨ë¸ ì‚­ì œ ì™„ë£Œ: {model_dir}")
                            return {"success": True, "message": "ëª¨ë¸ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
                    except Exception:
                        continue
        
        raise HTTPException(status_code=404, detail="ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

def analyze_uploaded_model(file_path: str, model_id: str, model_name: str, project_id: str = None):
    """ì—…ë¡œë“œëœ pkl íŒŒì¼ì—ì„œ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
    analysis_status = "success"
    error_message = None
    
    try:
        with open(file_path, 'rb') as f:
            model_data = pickle.load(f)

        logger.info(f"=== ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì‹œì‘: {model_name} ===")
        logger.info(f"ë°ì´í„° íƒ€ì…: {type(model_data)}")
        
        if isinstance(model_data, dict):
            logger.info(f"ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(model_data.keys())}")
            for key, value in model_data.items():
                logger.info(f"  {key}: {type(value)}")
        else:
            logger.info(f"ê°ì²´ ì†ì„±ë“¤: {dir(model_data)}")
        
        logger.info("=== ë¶„ì„ ì™„ë£Œ ===")
        
        
        # ê¸°ë³¸ê°’ (ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        performance = {
            "accuracy": None,
            "f1_score": None,
            "precision": None,
            "recall": None
        }
        
        config = {
            "backbone": None,
            "dataset_name": None,
            "epochs": None,
            "learning_method": None
        }
        
        # ì‹¤ì œ pkl ë°ì´í„° êµ¬ì¡° ë¶„ì„
        logger.info(f"ë¶„ì„ ì¤‘ì¸ ëª¨ë¸ ë°ì´í„° íƒ€ì…: {type(model_data)}")
        
        if isinstance(model_data, dict):
            logger.info(f"ëª¨ë¸ ë°ì´í„° í‚¤ë“¤: {list(model_data.keys())}")
            
            # ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            if 'performance' in model_data:
                perf_data = model_data['performance']
                if isinstance(perf_data, dict):
                    performance.update(perf_data)
                    logger.info("ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            elif 'metrics' in model_data:
                metrics_data = model_data['metrics']
                if isinstance(metrics_data, dict):
                    performance.update(metrics_data)
                    logger.info("ë©”íŠ¸ë¦­ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            elif 'test_results' in model_data:
                test_data = model_data['test_results']
                if isinstance(test_data, dict):
                    # test_resultsì—ì„œ ì„±ëŠ¥ ì§€í‘œ ë§¤í•‘
                    if 'image_auroc' in test_data:
                        performance['accuracy'] = test_data['image_auroc']
                    if 'pixel_auroc' in test_data:
                        performance['f1_score'] = test_data['pixel_auroc']
                    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            # ì„¤ì • ë°ì´í„° ì¶”ì¶œ ì‹œë„
            if 'config' in model_data:
                config_data = model_data['config']
                if isinstance(config_data, dict):
                    config.update(config_data)
                    logger.info("ì„¤ì • ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            # ë°±ë³¸ ì •ë³´ ì¶”ì¶œ
            if 'backbone' in model_data:
                config['backbone'] = model_data['backbone']
            elif 'model' in model_data and hasattr(model_data['model'], '__class__'):
                config['backbone'] = model_data['model'].__class__.__name__
            
            # PatchCore íŠ¹í™” ë¶„ì„
            if 'anomaly_maps' in model_data or 'patch_lib' in model_data:
                config['learning_method'] = 'PatchCore'
                logger.info("PatchCore ëª¨ë¸ë¡œ ì‹ë³„ë¨")
        
        else:
            # dictê°€ ì•„ë‹Œ ê²½ìš° (ëª¨ë¸ ê°ì²´ ë“±)
            logger.info(f"ëª¨ë¸ì´ dictê°€ ì•„ë‹˜. ê°ì²´ ë¶„ì„ ì‹œë„: {model_data.__class__.__name__ if hasattr(model_data, '__class__') else 'unknown'}")
            config['backbone'] = getattr(model_data, '__class__', {}).get('__name__', 'unknown')
            analysis_status = "partial"
            error_message = "ëª¨ë¸ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„"
        
        # ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        all_performance_none = all(v is None for v in performance.values())
        all_config_none = all(v is None for v in config.values())
        
        if all_performance_none and all_config_none:
            analysis_status = "failed"
            error_message = "ëª¨ë¸ì—ì„œ ì„±ëŠ¥ ë°ì´í„°ë‚˜ ì„¤ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        
        return {
            "id": model_id,
            "model_name": model_name,
            "project_id": project_id,
            "config": config,
            "performance": performance,
            "training_info": {
                "training_time": "ì—…ë¡œë“œë¨",
                "image_count": 0,
                "created_date": datetime.now().isoformat(),
                "file_size": Path(file_path).stat().st_size
            },
            "model_path": file_path,
            "status": "uploaded",
            "analysis_status": analysis_status,
            "analysis_error": error_message
        }
        
    except Exception as e:
        error_message = str(e)
        logger.error(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "id": model_id,
            "model_name": model_name,
            "project_id": project_id,
            "config": {
                "backbone": "ë¶„ì„ ì‹¤íŒ¨",
                "dataset_name": "ì—…ë¡œë“œë¨",
                "epochs": 0,
                "learning_method": "ë¶„ì„ ì‹¤íŒ¨"
            },
            "performance": {
                "accuracy": 0.0,
                "f1_score": 0.0,
                "precision": 0.0,
                "recall": 0.0
            },
            "training_info": {
                "training_time": "ì—…ë¡œë“œë¨",
                "image_count": 0,
                "created_date": datetime.now().isoformat()
            },
            "status": "analysis_failed",
            "analysis_status": "failed",
            "analysis_error": error_message
        }

# =============================================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    import subprocess
    import sys
    
    logger.info("ğŸš€ MLOps Vision Platform ì‹œì‘ ì¤‘...")
    logger.info("ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”")
    
    try:
        # uvicornì„ subprocessë¡œ ì‹¤í–‰ (ê°€ì¥ ì•ˆì •ì ì¸ ë°©ë²•)
        cmd = [
            sys.executable, "-m", "uvicorn", 
            "mlops_api:app", 
            "--host", "127.0.0.1", 
            "--port", "8080", 
            "--reload"
        ]
        
        logger.info("ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        subprocess.run(cmd)
        
    except KeyboardInterrupt:
        logger.info("ì„œë²„ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        logger.info("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        logger.info("uvicorn mlops_api:app --host 127.0.0.1 --port 8080 --reload")