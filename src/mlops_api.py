"""
MLOps Vision í†µí•© ì›¹ API
ëª¨ë“  PatchCore ê´€ë ¨ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©í•˜ëŠ” ë©”ì¸ API
"""

# FastAPI ê´€ë ¨
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, BackgroundTasks, Request
from fastapi.responses import HTMLResponse, FileResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# íƒ€ì… ê´€ë ¨
from typing import Dict, Any, List, Optional

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import json
import logging
import sqlite3
import uuid
import shutil
import pickle
import tempfile
import zipfile
from datetime import datetime
from pathlib import Path
import threading

from datetime import datetime, timezone, timedelta
KST = timezone(timedelta(hours=9))

# ìš°ë¦¬ê°€ ë§Œë“  ëª¨ë“ˆ
from patchcore_manager import PatchCoreManager
from dataset_manager import DatasetManager
from training_manager import PatchCoreTrainer, PatchCoreConfig
from model_manager import PatchCoreModelManager

# í…œí”Œë¦¿ (ì •ì  HTML ë Œë”ë§)
from fastapi.templating import Jinja2Templates


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="MLOps Vision Platform",
    description="Vision ê²€ì‚¬ MLOps í†µí•© í”Œë«í¼",
    version="1.0.0"
)

DB_PATH = "mlops_vision.db"

def dict_factory(cursor, row):
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d

def get_conn():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row  # â† ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì ‘ê·¼
    return conn

def now_kst_str():
    from datetime import datetime, timezone, timedelta
    KST = timezone(timedelta(hours=9))
    return datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S")  # â† DBì™€ ë™ì¼ í¬ë§·

def to_camel_case(row: dict) -> dict:
    mapping = {
        "project_id": "projectId",
        "dataset_id": "datasetId",
        "created_at": "createdAt",
        "updated_at": "updatedAt",
        "image_count": "imageCount",
        "total_size": "totalSize",
    }
    return {mapping.get(k, k): v for k, v in row.items()}

def recalc_dataset_stats(conn, project_id: str, dataset_id: str):
    """í´ë” ê¸°ì¤€ìœ¼ë¡œ image_count/total_size ì¬ê³„ì‚° í›„ DB ë°˜ì˜"""
    ds_dir = BASE_DATA_PATH / project_id / dataset_id
    ds_dir.mkdir(parents=True, exist_ok=True)
    count = 0
    total = 0
    if ds_dir.exists():
        for entry in ds_dir.iterdir():
            if entry.is_file():
                st = entry.stat()
                count += 1
                total += st.st_size
    cur = conn.cursor()
    cur.execute("""
        UPDATE project_datasets
        SET image_count=?, total_size=?, updated_at=CURRENT_TIMESTAMP
        WHERE id=? AND project_id=?
    """, (count, total, dataset_id, project_id))
    conn.commit()
    return count, total

# ì´ë¯¸ì§€ ëª©ë¡ ì¡°íšŒ (í†µì¼ëœ v2)
from typing import List  # íŒŒì¼ ìƒë‹¨ì— ì´ë¯¸ ìˆë‹¤ë©´ ì¤‘ë³µ import ë¬´ì‹œ

@app.get("/api/projects/{project_id}/datasets/{dataset_id}/images")
async def list_images_v2(project_id: str, dataset_id: str):
    try:
        conn = get_conn()
        cur = conn.cursor()

        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸
        cur.execute(
            "SELECT 1 FROM project_datasets WHERE id=? AND project_id=?",
            (dataset_id, project_id),
        )
        row = cur.fetchone()

        ds_dir = BASE_DATA_PATH / project_id / dataset_id

        # ğŸ”¥ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        print("=== [DEBUG] list_images_v2 ===")
        print(" project_id:", project_id)
        print(" dataset_id:", dataset_id)
        print(" DB row exists?:", bool(row))
        print(" ds_dir path:", ds_dir)
        print(" ds_dir.exists():", ds_dir.exists())
        print("================================")

        if not row:
            if ds_dir.exists():
                cur.execute(
                    "INSERT INTO project_datasets (id, project_id, name, path, type) VALUES (?, ?, ?, ?, ?)",
                    (dataset_id, project_id, dataset_id, str(ds_dir), "classify"),
                )
                conn.commit()
            else:
                conn.close()
                raise HTTPException(status_code=404, detail="ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        if not ds_dir.exists():
            conn.close()
            raise HTTPException(status_code=404, detail="ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        images = []
        for file_path in ds_dir.glob("*.*"):
            if file_path.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp", ".gif"]:
                stat = file_path.stat()
                images.append({
                    "name": file_path.name,
                    "size": stat.st_size,
                    "created_at": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    "updated_at": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                })

        conn.close()
        return {"images": images}

    except HTTPException:
        raise
    except Exception as e:
        print("list_images_v2 error:", e)
        raise HTTPException(status_code=500, detail=str(e))

# ì´ë¯¸ì§€ ì—…ë¡œë“œ
@app.post("/api/projects/{project_id}/datasets/{dataset_id}/images")
async def upload_images_v2(project_id: str, dataset_id: str, files: List[UploadFile] = File(...)):
    conn = None
    try:
        conn = get_conn()
        cur = conn.cursor()

        ds_dir = BASE_DATA_PATH / project_id / dataset_id
        ds_dir.mkdir(parents=True, exist_ok=True)

        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ í´ë” ê¸°ì¤€ìœ¼ë¡œ ìë™ ë“±ë¡) - path ë°˜ë“œì‹œ í¬í•¨
        cur.execute("SELECT 1 FROM project_datasets WHERE id=? AND project_id=?", (dataset_id, project_id))
        if not cur.fetchone():
            cur.execute("""
                INSERT INTO project_datasets
                    (id, project_id, name, path, image_count, created_at, type, description, total_size, updated_at)
                VALUES
                    (?,  ?,          ?,    ?,    0,           CURRENT_TIMESTAMP, ?,    ?,           0,          CURRENT_TIMESTAMP)
            """, (dataset_id, project_id, dataset_id, str(ds_dir), "classify", ""))
            conn.commit()

        # íŒŒì¼ ì €ì¥
        for file in files:
            dst = ds_dir / file.filename
            with open(dst, "wb") as f:
                shutil.copyfileobj(file.file, f)

        # í†µê³„ ê°±ì‹ 
        count, total = recalc_dataset_stats(conn, project_id, dataset_id)

        # ì—…ë¡œë“œ í›„ ìµœì‹  ì´ë¯¸ì§€ ëª©ë¡ ìƒì„± (í”„ë¡ íŠ¸ê°€ ê¸°ëŒ€í•˜ëŠ” êµ¬ì¡°)
        images = []
        for file_path in ds_dir.glob("*.*"):
            if file_path.suffix.lower() in [".jpg", ".jpeg", ".png", ".bmp", ".gif"]:
                st = file_path.stat()
                images.append({
                    "name": file_path.name,
                    "size": st.st_size,
                    "created_at": datetime.fromtimestamp(st.st_ctime).isoformat(),
                    "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
                })

        conn.commit()
        return {"success": True, "images": images, "imageCount": count, "totalSize": total}

    except HTTPException:
        if conn: conn.close()
        raise
    except Exception as e:
        if conn: conn.close()
        logger.error(f"ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    finally:
        if conn:
            conn.close()

# Static íŒŒì¼ê³¼ í…œí”Œë¦¿ ì„¤ì •
app.mount("/static", StaticFiles(directory="../static"), name="static")
templates = Jinja2Templates(directory="../templates")

# ì „ì—­ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ë“¤
patchcore_manager = PatchCoreManager()
dataset_manager = DatasetManager()
trainer = PatchCoreTrainer()
model_manager = PatchCoreModelManager()
BASE_DATA_PATH = Path(__file__).resolve().parent / "data" / "processed"

def init_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    conn = sqlite3.connect('mlops_vision.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS projects (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL UNIQUE,
            description TEXT,
            status TEXT DEFAULT 'active',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # ë°ì´í„°ì…‹ í…Œì´ë¸” (í”„ë¡œì íŠ¸ ì—°ê²°)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS project_datasets (
            id TEXT PRIMARY KEY,
            project_id TEXT NOT NULL,
            name TEXT NOT NULL,
            path TEXT NOT NULL,
            image_count INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES projects (id)
        )
    ''')

     # ëˆ„ë½ ì»¬ëŸ¼ ì¶”ê°€
    def add_column_if_missing(table, column_def):
        col_name = column_def.split()[0]
        cursor.execute(f"PRAGMA table_info({table})")
        cols = [r[1] for r in cursor.fetchall()]
        if col_name not in cols:
            cursor.execute(f"ALTER TABLE {table} ADD COLUMN {column_def}")

    # datasets í™•ì¥ ì»¬ëŸ¼
    add_column_if_missing("project_datasets", "type TEXT")
    add_column_if_missing("project_datasets", "description TEXT")
    add_column_if_missing("project_datasets", "total_size INTEGER DEFAULT 0")

    # â—DEFAULT CURRENT_TIMESTAMPëŠ” í—ˆìš© ì•ˆ ë¨ â†’ ê¸°ë³¸ê°’ ì—†ì´ ì¶”ê°€
    add_column_if_missing("project_datasets", "updated_at TIMESTAMP")

    # âœ… ê¸°ì¡´ í–‰ ë³´ì •(ë„ì´ë©´ í˜„ì¬ ì‹œê°ìœ¼ë¡œ)
    cursor.execute("""
        UPDATE project_datasets
        SET updated_at = COALESCE(updated_at, CURRENT_TIMESTAMP)
    """)

    # âœ… íŠ¸ë¦¬ê±°ë¡œ updated_at ìë™ ê°±ì‹ 
    cursor.execute("""
        CREATE TRIGGER IF NOT EXISTS trg_project_datasets_set_updated_at_on_update
        AFTER UPDATE ON project_datasets
        FOR EACH ROW
        BEGIN
            UPDATE project_datasets
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = NEW.id;
        END;
    """)

    cursor.execute("""
        CREATE TRIGGER IF NOT EXISTS trg_project_datasets_set_updated_at_on_insert
        AFTER INSERT ON project_datasets
        FOR EACH ROW
        WHEN NEW.updated_at IS NULL
        BEGIN
            UPDATE project_datasets
            SET updated_at = CURRENT_TIMESTAMP
            WHERE id = NEW.id;
        END;
    """)
    
    # ëª¨ë¸ í…Œì´ë¸” (í”„ë¡œì íŠ¸ ì—°ê²°)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS project_models (
            id TEXT PRIMARY KEY,
            project_id TEXT NOT NULL,
            dataset_id TEXT NOT NULL,
            name TEXT NOT NULL,
            backbone TEXT,
            performance_data TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES projects (id),
            FOREIGN KEY (dataset_id) REFERENCES project_datasets (id)
        )
    ''')
    
    conn.commit()
    conn.close()

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ DB ì´ˆê¸°í™”
init_database()

# Pydantic ëª¨ë¸ ì •ì˜ (API ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ)
class TrainingConfigModel(BaseModel):
    """í•™ìŠµ ì„¤ì •ì„ ìœ„í•œ Pydantic ëª¨ë¸"""
    backbone: str = "wideresnet50"
    layers: List[str] = ["layer2", "layer3"]
    anomaly_scorer_num_nn: int = 7
    sampling_ratio: float = 0.3
    dataset_name: str  # í•„ìˆ˜ í•„ë“œë¡œ ë³€ê²½
    resize_size: int = 320
    image_size: int = 256
    gpu_id: Optional[int] = None
    
    # ìƒˆë¡œ ì¶”ê°€ëœ í•„ë“œë“¤
    training_type: str = "new"  # "new", "transfer", "retrain"
    base_model: Optional[str] = None  # ì „ì´í•™ìŠµ/ì¬í•™ìŠµ ì‹œ ê¸°ì¡´ ëª¨ë¸ëª…
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 10
    train_ratio: float = 0.7
    val_ratio: float = 0.2
    test_ratio: float = 0.1

class SystemStatusResponse(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""
    patchcore_installed: bool
    datasets_available: int
    models_trained: int
    training_active: bool
    system_ready: bool

# =============================================================================
# ì‹œìŠ¤í…œ ìƒíƒœ ë° ì´ˆê¸°í™” API
# =============================================================================


@app.get("/", response_class=HTMLResponse)
async def redirect_to_dashboard():
    """ë©”ì¸ í˜ì´ì§€ì—ì„œ ëŒ€ì‹œë³´ë“œë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸"""
    return RedirectResponse(url="/dashboard")

@app.get("/dashboard", response_class=HTMLResponse)
async def get_dashboard(request: Request):
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í˜ì´ì§€"""
    return templates.TemplateResponse("dashboard.html", {"request": request})

@app.get("/ui/datasets", response_class=HTMLResponse)
async def get_datasets_page(request: Request):
    """ë°ì´í„°ì…‹ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("datasets.html", {"request": request})

@app.get("/ui/training", response_class=HTMLResponse)
async def get_training_page(request: Request):
    """í•™ìŠµ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("training.html", {"request": request})

@app.get("/ui/models", response_class=HTMLResponse)
async def get_models_page(request: Request):
    """ëª¨ë¸ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("models.html", {"request": request})

@app.get("/ui/projects", response_class=HTMLResponse)
async def get_projects_page(request: Request):
    """í”„ë¡œì íŠ¸ ê´€ë¦¬ í˜ì´ì§€"""
    return templates.TemplateResponse("projects.html", {"request": request})

@app.get("/ui/monitoring", response_class=HTMLResponse)
async def get_monitoring_page(request: Request):
    """ëª¨ë‹ˆí„°ë§ í˜ì´ì§€"""
    return templates.TemplateResponse("monitoring.html", {"request": request})

@app.get("/system/status", response_model=SystemStatusResponse)
async def get_system_status():
    """ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        # PatchCore ì„¤ì¹˜ ìƒíƒœ í™•ì¸
        patchcore_installed = patchcore_manager.is_patchcore_installed()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìˆ˜ í™•ì¸
        datasets = dataset_manager.list_available_datasets()
        datasets_count = len(datasets)
        
        # í•™ìŠµëœ ëª¨ë¸ ìˆ˜ í™•ì¸
        models = model_manager.discover_trained_models()
        models_count = len(models)
        
        # í˜„ì¬ í•™ìŠµ ì§„í–‰ ìƒíƒœ í™•ì¸
        training_active = trainer.is_training_active()
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœ
        system_ready = patchcore_installed and datasets_count > 0
        
        return SystemStatusResponse(
            patchcore_installed=patchcore_installed,
            datasets_available=datasets_count,
            models_trained=models_count,
            training_active=training_active,
            system_ready=system_ready
        )
        
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/system/setup")
async def setup_system():
    """ì „ì²´ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. (ì´ë¯¸ ì„¤ì¹˜ëœ êµ¬ì„±ìš”ì†ŒëŠ” ê±´ë„ˆëœë‹ˆë‹¤)"""
    try:
        logger.info("ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...")
        
        setup_results = {
            "patchcore_status": "unknown",
            "actions_taken": [],
            "skipped_actions": [],
            "warnings": []
        }
        
        # 1. PatchCore ì„¤ì¹˜ ìƒíƒœ í™•ì¸
        if patchcore_manager.is_patchcore_installed():
            setup_results["patchcore_status"] = "already_installed"
            setup_results["skipped_actions"].append("PatchCore ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì´ë¯¸ ì„¤ì¹˜ë¨)")
            logger.info("âœ… PatchCoreê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            
            # ì¶”ê°€ ê²€ì¦ë§Œ ìˆ˜í–‰
            verification = patchcore_manager.verify_installation()
            if not verification["installation_complete"]:
                setup_results["warnings"].append("PatchCore ì„¤ì¹˜ê°€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                setup_results["warnings"].extend(verification["error_messages"])
        else:
            logger.info("ğŸ“¦ PatchCore ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤...")
            setup_result = patchcore_manager.setup_patchcore_complete()
            
            if setup_result:
                setup_results["patchcore_status"] = "newly_installed"
                setup_results["actions_taken"].append("PatchCore ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ")
            else:
                setup_results["patchcore_status"] = "installation_failed"
                setup_results["warnings"].append("PatchCore ì„¤ì¹˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # 2. ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™•ì¸ ë° ìƒì„±
        required_dirs = [
            dataset_manager.data_dir,
            dataset_manager.processed_data_dir,
            trainer.results_dir,
            model_manager.models_archive_dir
        ]
        
        dirs_created = []
        for dir_path in required_dirs:
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                dirs_created.append(str(dir_path.name))
        
        if dirs_created:
            setup_results["actions_taken"].append(f"ë””ë ‰í† ë¦¬ ìƒì„±: {', '.join(dirs_created)}")
        else:
            setup_results["skipped_actions"].append("ëª¨ë“  í•„ìˆ˜ ë””ë ‰í† ë¦¬ (ì´ë¯¸ ì¡´ì¬)")
        
        # 3. ì‹œìŠ¤í…œ ì „ì²´ ìƒíƒœ í™•ì¸
        system_ready = setup_results["patchcore_status"] in ["already_installed", "newly_installed"]
        
        # 4. ê²°ê³¼ ì •ë¦¬
        total_actions = len(setup_results["actions_taken"])
        total_skipped = len(setup_results["skipped_actions"])
        
        if system_ready:
            logger.info("âœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
            return {
                "success": True,
                "system_ready": True,
                "message": f"ì‹œìŠ¤í…œì´ ì‚¬ìš© ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ({total_actions}ê°œ ì‘ì—… ìˆ˜í–‰, {total_skipped}ê°œ ì‘ì—… ê±´ë„ˆëœ€)",
                "details": setup_results,
                "next_steps": [
                    "ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (/datasets/upload)",
                    "í•™ìŠµ ì„¤ì •ì„ êµ¬ì„±í•˜ì„¸ìš” (/training/start)", 
                    "ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ì„¸ìš”"
                ]
            }
        else:
            logger.error("âŒ ì‹œìŠ¤í…œ ì„¤ì •ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤")
            return {
                "success": False,
                "system_ready": False,
                "message": "ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "details": setup_results,
                "troubleshooting": [
                    "ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ êµ¬ì²´ì ì¸ ì˜¤ë¥˜ë¥¼ íŒŒì•…í•˜ì„¸ìš”",
                    "í•„ìš”í•œ ê²½ìš° ìˆ˜ë™ìœ¼ë¡œ PatchCoreë¥¼ ì„¤ì¹˜í•´ë³´ì„¸ìš”",
                    "ì‹œìŠ¤í…œ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”"
                ]
            }
            
    except Exception as e:
        logger.error(f"ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return {
            "success": False,
            "system_ready": False,
            "error": str(e),
            "message": "ì‹œìŠ¤í…œ ì„¤ì • ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }
    
@app.post("/system/setup/force-reinstall")
async def force_reinstall_system():
    """ê°•ì œë¡œ ì „ì²´ ì‹œìŠ¤í…œì„ ì¬ì„¤ì¹˜í•©ë‹ˆë‹¤. (ì£¼ì˜: ê¸°ì¡´ ì„¤ì¹˜ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤)"""
    try:
        logger.warning("âš ï¸ ê°•ì œ ì¬ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ê°•ì œ ì¬ì„¤ì¹˜ ìˆ˜í–‰
        setup_result = patchcore_manager.setup_patchcore_complete(force_reinstall=True)
        
        if setup_result:
            logger.info("âœ… ê°•ì œ ì¬ì„¤ì¹˜ ì™„ë£Œ")
            return {
                "success": True,
                "message": "ì‹œìŠ¤í…œì´ ê°•ì œë¡œ ì¬ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "warning": "ëª¨ë“  ê¸°ì¡´ ì„¤ì •ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            }
        else:
            raise HTTPException(status_code=500, detail="ê°•ì œ ì¬ì„¤ì¹˜ ì‹¤íŒ¨")
            
    except Exception as e:
        logger.error(f"ê°•ì œ ì¬ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# í”„ë¡œì íŠ¸ ê´€ë¦¬ API
# =============================================================================

# í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ
@app.get("/api/projects")
async def list_projects():
    conn = None
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = dict_factory
        cur = conn.cursor()

        # í”„ë¡œì íŠ¸ ê¸°ë³¸ ëª©ë¡
        cur.execute("SELECT * FROM projects ORDER BY created_at ASC")
        p_rows = cur.fetchall()

        result = []
        for p in p_rows:
            project = to_camel_case(p)

            # ë‚ ì§œ ë³´ì • (â€“ ë°©ì§€)
            created_at_raw = p.get("created_at")
            updated_at_raw = p.get("updated_at")
            now = now_kst_str()
            project["createdAt"] = project.get("createdAt") or created_at_raw or now
            project["updatedAt"] = project.get("updatedAt") or updated_at_raw or project["createdAt"]

            # ë°ì´í„°ì…‹ ëª©ë¡ (* ë¡œ ì•ˆì „ ì¡°íšŒ)
            cur.execute("SELECT * FROM project_datasets WHERE project_id=?", (p["id"],))
            ds_rows = cur.fetchall()

            dataset_list = []
            for d in ds_rows:
                item = {
                    "id": d.get("id"),
                    "name": d.get("name"),
                    "type": (d.get("type") or "classify"),
                    "createdAt": d.get("created_at"),
                    "imageCount": d.get("image_count", 0),
                    "totalSize": d.get("total_size", 0),
                    "description": d.get("description") or ""
                }
                # ê³¼ê±° í…œí”Œë¦¿ í˜¸í™˜ìš© ë³„ì¹­
                item["date"] = item["createdAt"]
                dataset_list.append(item)

            project["datasetList"] = dataset_list
            project["datasetCount"] = len(dataset_list)

            # ëª¨ë¸ ê°œìˆ˜ (í…Œì´ë¸” ì—†ê±°ë‚˜ ì˜¤ë¥˜ì—¬ë„ 0)
            try:
                cur.execute("SELECT COUNT(*) AS cnt FROM project_models WHERE project_id=?", (p["id"],))
                mc = cur.fetchone()
                project["modelCount"] = mc.get("cnt", 0) if mc else 0
            except Exception:
                project["modelCount"] = 0

            result.append(project)

        return result
    except Exception as e:
        logger.error(f"/api/projects ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if conn:
            conn.close()
    
# í”„ë¡œì íŠ¸ ìƒì„±
@app.post("/api/projects")
async def create_project(project_data: dict):
    try:
        project_id = str(uuid.uuid4())
        name = project_data.get("name")
        description = project_data.get("description", "")
        status = project_data.get("status", "active")
        if not name:
            raise HTTPException(status_code=400, detail="í”„ë¡œì íŠ¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤")

        now = now_kst_str()

        conn = get_conn()
        cur = conn.cursor()
        # ì»¬ëŸ¼ì„ ëª…ì‹œí•˜ê³ , ìš°ë¦¬ê°€ ë„£ì„ ê°’ê³¼ ìˆœì„œë¥¼ 1:1ë¡œ ë§ì¶¥ë‹ˆë‹¤.
        cur.execute("""
            INSERT INTO projects (id, name, description, created_at, updated_at, status)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (project_id, name, description, now, now, status))

        conn.commit()
        cur.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        row = cur.fetchone()
        conn.close()

        project = {
            "id": row["id"],
            "name": row["name"],
            "description": row["description"],
            "status": row["status"],
            "created_at": row["created_at"],
            "updated_at": row["updated_at"],
        }
        return {"success": True, "project": project}

    except sqlite3.IntegrityError:
        raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í”„ë¡œì íŠ¸ ì´ë¦„ì…ë‹ˆë‹¤")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# í”„ë¡œì íŠ¸ ìˆ˜ì •
@app.put("/api/projects/{project_id}")
async def update_project(project_id: str, project_data: dict):
    try:
        name = project_data.get("name")
        description = project_data.get("description", "")
        status = project_data.get("status", "active")
        if not name:
            raise HTTPException(status_code=400, detail="í”„ë¡œì íŠ¸ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤")

        now = now_kst_str()
        conn = get_conn()
        cur = conn.cursor()

        cur.execute("""
            UPDATE projects
               SET name=?, description=?, status=?, updated_at=?
             WHERE id=?
        """, (name, description, status, now, project_id))

        if cur.rowcount == 0:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        conn.commit()
        cur.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        row = cur.fetchone()
        conn.close()

        project = {
            "id": row["id"],
            "name": row["name"],
            "description": row["description"],
            "status": row["status"],
            "created_at": row["created_at"],
            "updated_at": row["updated_at"],
        }
        return {"success": True, "project": project}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
# í”„ë¡œì íŠ¸ ì‚­ì œ
@app.delete("/api/projects/{project_id}")
async def delete_project(project_id: str):
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()

        cursor.execute("DELETE FROM projects WHERE id=?", (project_id,))
        if cursor.rowcount == 0:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        conn.commit()
        conn.close()
        return {"success": True, "message": "í”„ë¡œì íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
# í”„ë¡œì íŠ¸ ë‚´ë³´ë‚´ê¸°
@app.get("/api/projects/{project_id}/export")
async def export_project(project_id: str, mode: str = "meta"):
    """
    í”„ë¡œì íŠ¸ ë‚´ë³´ë‚´ê¸°
    - mode=meta : project.json + datasets.json + models/zip
    - mode=full : project.json + datasets/zip + models/zip
    """
    try:
        conn = get_conn()
        cur = conn.cursor()

        # 1. í”„ë¡œì íŠ¸ ì¡°íšŒ
        cur.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        p_row = cur.fetchone()
        if not p_row:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        project = {
            "id": p_row["id"],
            "name": p_row["name"],
            "description": p_row["description"],
            "status": p_row["status"],
            "created_at": p_row["created_at"],
            "updated_at": p_row["updated_at"],
            "models": []
        }

        # 2. ë°ì´í„°ì…‹ ì¡°íšŒ
        cur.execute("SELECT * FROM project_datasets WHERE project_id=?", (project_id,))
        datasets = [{
            "id": ds["id"],
            "project_id": ds["project_id"],
            "name": ds["name"],
            "path": ds["path"],
            "image_count": ds["image_count"],
            "created_at": ds["created_at"],
        } for ds in cur.fetchall()]

        # 3. ëª¨ë¸ ì¡°íšŒ
        cur.execute("SELECT * FROM project_models WHERE project_id=?", (project_id,))
        for m in cur.fetchall():
            project["models"].append({
                "id": m["id"],
                "project_id": m["project_id"],
                "dataset_id": m["dataset_id"],
                "name": m["name"],
                "backbone": m["backbone"],
                "performance_data": m["performance_data"],
                "created_at": m["created_at"]
            })

        conn.close()

        # 4. ì„ì‹œ zip ìƒì„±
        tmpdir = tempfile.mkdtemp()
        zip_path = os.path.join(tmpdir, f"{project['name']}_export_{mode}.zip")
        with zipfile.ZipFile(zip_path, "w") as zipf:
            # project.json
            zipf.writestr("project.json", json.dumps(project, indent=2, ensure_ascii=False))

            if mode == "meta":
                # datasets.json
                zipf.writestr("datasets.json", json.dumps(datasets, indent=2, ensure_ascii=False))

            elif mode == "full":
                # datasetë³„ zip ìƒì„±
                for ds in datasets:
                    src_path = ds["path"]
                    if os.path.exists(src_path):
                        ds_zip_name = f"datasets/{ds['id']}.zip"
                        with tempfile.TemporaryDirectory() as ds_tmp:
                            ds_zip = os.path.join(ds_tmp, f"{ds['id']}.zip")
                            with zipfile.ZipFile(ds_zip, "w") as dsf:
                                for root, _, files in os.walk(src_path):
                                    for f in files:
                                        abs_path = os.path.join(root, f)
                                        rel_path = os.path.relpath(abs_path, src_path)
                                        dsf.write(abs_path, rel_path)
                            zipf.write(ds_zip, ds_zip_name)

            # models
            for m in project["models"]:
                src_model_path = os.path.join("saved_models", m["id"])
                if os.path.exists(src_model_path):
                    model_zip_name = f"models/{m['id']}.zip"
                    with tempfile.TemporaryDirectory() as m_tmp:
                        m_zip = os.path.join(m_tmp, f"{m['id']}.zip")
                        with zipfile.ZipFile(m_zip, "w") as mf:
                            for root, _, files in os.walk(src_model_path):
                                for f in files:
                                    abs_path = os.path.join(root, f)
                                    rel_path = os.path.relpath(abs_path, src_model_path)
                                    mf.write(abs_path, rel_path)
                        zipf.write(m_zip, model_zip_name)

        return FileResponse(zip_path, filename=os.path.basename(zip_path))

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/projects/import")
async def import_project(file: UploadFile = File(...)):
    """
    í”„ë¡œì íŠ¸ ê°€ì ¸ì˜¤ê¸° (Meta Export ë˜ëŠ” Full Export zip ì—…ë¡œë“œ)
    """
    import uuid, os, zipfile, shutil, json, tempfile

    try:
        # 1. ì—…ë¡œë“œ zip ì €ì¥
        upload_dir = "uploads"
        os.makedirs(upload_dir, exist_ok=True)
        zip_path = os.path.join(upload_dir, file.filename)
        with open(zip_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # 2. ì••ì¶• í•´ì œ
        extract_dir = tempfile.mkdtemp()
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_dir)

        # 3. project.json ë¡œë“œ
        project_json_path = os.path.join(extract_dir, "project.json")
        if not os.path.exists(project_json_path):
            raise HTTPException(status_code=400, detail="project.jsonì´ ì—†ìŠµë‹ˆë‹¤")

        with open(project_json_path, "r", encoding="utf-8") as f:
            project_data = json.load(f)

        # ìƒˆ í”„ë¡œì íŠ¸ ID
        new_project_id = str(uuid.uuid4())
        now = now_kst_str()

        conn = get_conn()
        cur = conn.cursor()

        # 4. í”„ë¡œì íŠ¸ ì‚½ì… (created_atì€ ì›ë³¸ ìœ ì§€, updated_atì€ í˜„ì¬ ì‹œê°)
        cur.execute("""
            INSERT INTO projects (id, name, description, created_at, updated_at, status)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            new_project_id,
            project_data.get("name") + " (ê°€ì ¸ì˜¤ê¸°)",
            project_data.get("description", ""),
            project_data.get("created_at"),  # ì›ë³¸
            now,                             # í˜„ì¬ ì‹œê°
            project_data.get("status", "active")
        ))

        dataset_id_map = {}

        # 5. datasets.json ì²˜ë¦¬ (meta export)
        datasets_json_path = os.path.join(extract_dir, "datasets.json")
        if os.path.exists(datasets_json_path):
            with open(datasets_json_path, "r", encoding="utf-8") as f:
                datasets_data = json.load(f)

            for ds in datasets_data:
                new_dataset_id = str(uuid.uuid4())
                dataset_id_map[ds["id"]] = new_dataset_id

                ds_zip_path = os.path.join(extract_dir, "datasets", f"{ds['id']}.zip")
                dst_path = os.path.join("processed", new_project_id, new_dataset_id)
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)

                if os.path.exists(ds_zip_path):
                    with zipfile.ZipFile(ds_zip_path, "r") as ds_zip:
                        ds_zip.extractall(dst_path)

                cur.execute("""
                    INSERT INTO project_datasets (id, project_id, name, path, image_count, created_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (
                    new_dataset_id,
                    new_project_id,
                    ds.get("name"),
                    dst_path if os.path.exists(ds_zip_path) else ds.get("path"),
                    ds.get("image_count", 0),
                    ds.get("created_at")  # ì›ë³¸ ìƒì„±ì¼ ìœ ì§€
                ))

        # 6. ëª¨ë¸ ë³µì›
        if "models" in project_data:
            for m in project_data["models"]:
                new_model_id = str(uuid.uuid4())
                new_dataset_id = dataset_id_map.get(m["dataset_id"], None)

                cur.execute("""
                    INSERT INTO project_models (id, project_id, dataset_id, name, backbone, performance_data, created_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    new_model_id,
                    new_project_id,
                    new_dataset_id,
                    m.get("name"),
                    m.get("backbone"),
                    m.get("performance_data"),
                    m.get("created_at")  # ì›ë³¸ ìƒì„±ì¼ ìœ ì§€
                ))

                # ëª¨ë¸ íŒŒì¼ ë³µì›
                src_model_zip = os.path.join(extract_dir, "models", f"{m['id']}.zip")
                dst_model_path = os.path.join("saved_models", new_model_id)
                if os.path.exists(src_model_zip):
                    os.makedirs(dst_model_path, exist_ok=True)
                    with zipfile.ZipFile(src_model_zip, "r") as mzip:
                        mzip.extractall(dst_model_path)

        conn.commit()
        conn.close()

        return {"success": True, "new_project_id": new_project_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/projects/{project_id}/duplicate")
async def duplicate_project(project_id: str):
    """
    í”„ë¡œì íŠ¸ Deep Copy (í”„ë¡œì íŠ¸ + ë°ì´í„°ì…‹ + ì´ë¯¸ì§€ + ëª¨ë¸ + ëª¨ë¸ íŒŒì¼)
    - ì»¬ëŸ¼ëª… ì ‘ê·¼ìœ¼ë¡œ ì¸ë±ìŠ¤ ê¼¬ì„ ë°©ì§€
    - created_at/updated_at: KST "YYYY-MM-DD HH:MM:SS"
    """
    import os, shutil, uuid
    try:
        conn = get_conn()
        cur = conn.cursor()

        # 1) ì›ë³¸ í”„ë¡œì íŠ¸
        cur.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        proj = cur.fetchone()
        if not proj:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        new_project_id = str(uuid.uuid4())
        new_project_name = f'{proj["name"]} (ë³µì‚¬ë³¸)'
        status_value = proj["status"] or "active"
        now = now_kst_str()

        # 2) ìƒˆ í”„ë¡œì íŠ¸ insert (ì»¬ëŸ¼ ìˆœì„œ ëª…ì‹œ)
        cur.execute("""
            INSERT INTO projects (id, name, description, created_at, updated_at, status)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (new_project_id, new_project_name, proj["description"], now, now, status_value))

        # 3) ë°ì´í„°ì…‹ ë³µì œ
        cur.execute("SELECT * FROM project_datasets WHERE project_id=?", (project_id,))
        ds_rows = cur.fetchall()
        ds_id_map = {}
        for ds in ds_rows:
            new_ds_id = str(uuid.uuid4())
            ds_id_map[ds["id"]] = new_ds_id

            src_path = ds["path"]
            dst_path = src_path.replace(project_id, new_project_id)

            cur.execute("""
                INSERT INTO project_datasets (id, project_id, name, path, image_count, created_at)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (new_ds_id, new_project_id, ds["name"], dst_path, ds["image_count"], now))

            if os.path.exists(src_path):
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                shutil.copytree(src_path, dst_path)

        # 4) ëª¨ë¸ ë³µì œ
        cur.execute("SELECT * FROM project_models WHERE project_id=?", (project_id,))
        mdl_rows = cur.fetchall()
        for m in mdl_rows:
            new_model_id = str(uuid.uuid4())
            new_ds_id = ds_id_map.get(m["dataset_id"], m["dataset_id"])

            cur.execute("""
                INSERT INTO project_models (id, project_id, dataset_id, name, backbone, performance_data, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (new_model_id, new_project_id, new_ds_id, m["name"], m["backbone"], m["performance_data"], now))

            src_model_path = os.path.join("saved_models", m["id"])
            dst_model_path = os.path.join("saved_models", new_model_id)
            if os.path.exists(src_model_path):
                shutil.copytree(src_model_path, dst_model_path)

        conn.commit()
        conn.close()
        return {"success": True, "new_project_id": new_project_id, "new_name": new_project_name}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/projects/{project_id}")
async def get_project_detail(project_id: str):
    try:
        conn = get_conn()
        cur = conn.cursor()

        cur.execute("SELECT * FROM projects WHERE id=?", (project_id,))
        p = cur.fetchone()
        if not p:
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        cur.execute("SELECT * FROM project_datasets WHERE project_id=?", (project_id,))
        datasets = [to_camel_case(dict(ds)) for ds in cur.fetchall()]

        cur.execute("SELECT * FROM project_models WHERE project_id=?", (project_id,))
        models = [to_camel_case(dict(m)) for m in cur.fetchall()]

        project = to_camel_case(dict(p))
        project["datasets"] = datasets
        project["models"] = models

        conn.close()
        return {"success": True, "project": project}
    except Exception as e:
        logger.error(f"í”„ë¡œì íŠ¸ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# ë°ì´í„°ì…‹ ê´€ë¦¬ API
# =============================================================================

@app.get("/datasets")
async def list_datasets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        datasets = dataset_manager.list_available_datasets()
        return {
            "datasets": datasets,
            "total_count": len(datasets)
        }
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.get("/api/projects/{project_id}/datasets")
async def list_project_datasets(project_id: str):
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = dict_factory
        cur = conn.cursor()

        # í”„ë¡œì íŠ¸ ì¡´ì¬ í™•ì¸
        cur.execute("SELECT id FROM projects WHERE id=?", (project_id,))
        if not cur.fetchone():
            conn.close()
            raise HTTPException(status_code=404, detail="í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ë°ì´í„°ì…‹ ì¡°íšŒ(*ë¡œ ì•ˆì „ ì¡°íšŒ) â†’ camelCase
        cur.execute("""
            SELECT * FROM project_datasets
            WHERE project_id=?
            ORDER BY created_at ASC
        """, (project_id,))
        rows = cur.fetchall()
        conn.close()

        # camelCase + ê¸°ë³¸ê°’ ë³´ì •
        datasets = []
        for r in rows:
            r_c = to_camel_case(r)
            r_c.setdefault("type", "classify")
            r_c.setdefault("description", "")
            r_c.setdefault("totalSize", 0)
            return_map = {
                **r_c,
                "imageCount": r_c.get("imageCount", 0)
            }
            datasets.append(return_map)

        return datasets
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/api/projects/{project_id}/datasets")
async def create_dataset(project_id: str, dataset_data: dict):
    try:
        name = dataset_data.get("name", "").strip()
        dtype = dataset_data.get("type", "classify")
        description = dataset_data.get("description", "").strip()
        if not name:
            raise HTTPException(status_code=400, detail="ë°ì´í„°ì…‹ ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤")

        new_id = str(uuid.uuid4())
        now = now_kst_str()
        ds_path = BASE_DATA_PATH / project_id / new_id
        ds_path.mkdir(parents=True, exist_ok=True)

        conn = sqlite3.connect(DB_PATH)
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO project_datasets
            (id, project_id, name, path, image_count, created_at, type, description, total_size, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (new_id, project_id, name, str(ds_path), 0, now, dtype, description, 0, now))
        conn.commit()
        conn.close()

        return {
            "success": True,
            "dataset": {
                "id": new_id,
                "projectId": project_id,
                "name": name,
                "type": dtype,
                "description": description,
                "imageCount": 0,
                "totalSize": 0,
                "createdAt": now,
                "updatedAt": now
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/projects/{project_id}/datasets/{dataset_id}")
async def delete_dataset(project_id: str, dataset_id: str):
    """
    ë°ì´í„°ì…‹ ì‚­ì œ:
    - íŒŒì¼ì‹œìŠ¤í…œ: {BASE_DATA_PATH}/{project_id}/{dataset_id} ë””ë ‰í† ë¦¬ ì‚­ì œ
    - DB: project_datasets í–‰ ì‚­ì œ
    """
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = dict_factory
        cur = conn.cursor()

        # ì¡´ì¬ í™•ì¸ + ê²½ë¡œ ì¡°íšŒ
        cur.execute(
            "SELECT path FROM project_datasets WHERE id=? AND project_id=?",
            (dataset_id, project_id)
        )
        row = cur.fetchone()
        if not row:
            conn.close()
            raise HTTPException(status_code=404, detail="ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # í´ë” ì‚­ì œ (ê²½ë¡œê°€ DBì— ì—†ìœ¼ë©´ ê¸°ë³¸ ê·œì¹™ìœ¼ë¡œ ì¶”ì •)
        ds_path = Path(row.get("path") or (BASE_DATA_PATH / project_id / dataset_id))
        try:
            if ds_path.exists():
                shutil.rmtree(ds_path, ignore_errors=True)
        except Exception as fe:
            # í´ë” ì‚­ì œ ì‹¤íŒ¨í•´ë„ DB ì •ë¦¬ ê°€ëŠ¥í•˜ë„ë¡ ë¡œê·¸ë§Œ ë‚¨ê¹€
            logger.error(f"ë°ì´í„°ì…‹ í´ë” ì‚­ì œ ì‹¤íŒ¨: {fe}")

        # DB ì‚­ì œ
        cur.execute(
            "DELETE FROM project_datasets WHERE id=? AND project_id=?",
            (dataset_id, project_id)
        )
        conn.commit()
        conn.close()

        return {"success": True}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
# í†µì¼ëœ ì´ë¯¸ì§€ ì‚­ì œ API (ê¶Œì¥ ì—”ë“œí¬ì¸íŠ¸)
@app.delete("/api/projects/{project_id}/datasets/{dataset_id}/images/{filename}")
async def delete_image_v2(project_id: str, dataset_id: str, filename: str):
    try:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = dict_factory
        cur = conn.cursor()

        ds_dir = BASE_DATA_PATH / project_id / dataset_id
        ds_dir.mkdir(parents=True, exist_ok=True)

        # âœ… ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ â†’ ì—†ìœ¼ë©´ í´ë”ê°€ ìˆì„ ë•Œ ìë™ ë“±ë¡
        cur.execute("SELECT 1 FROM project_datasets WHERE id=? AND project_id=?", (dataset_id, project_id))
        if not cur.fetchone():
            if ds_dir.exists():
                cur.execute("""
                    INSERT INTO project_datasets (id, project_id, name, type, created_at, updated_at, image_count, total_size, description)
                    VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, 0, 0, ?)
                """, (dataset_id, project_id, dataset_id, "classify", ""))
                conn.commit()
            else:
                conn.close()
                raise HTTPException(status_code=404, detail="ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        file_path = ds_dir / filename
        if not file_path.exists():
            conn.close()
            raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # íŒŒì¼ ì‚­ì œ
        try:
            file_path.unlink()
        except Exception as fe:
            conn.close()
            raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {fe}")

        # í†µê³„ ê°±ì‹ 
        count, total = recalc_dataset_stats(conn, project_id, dataset_id)
        conn.commit()
        conn.close()

        return {"success": True, "imageCount": count, "totalSize": total}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

@app.post("/datasets/upload")
async def upload_dataset(
    file: UploadFile = File(...),
    dataset_name: str = Form(...)
):
    """ë°ì´í„°ì…‹ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        logger.info(f"ğŸ“¤ ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹œì‘: {file.filename}")
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        try:
            # ë°ì´í„°ì…‹ ì²˜ë¦¬
            result = dataset_manager.process_uploaded_dataset(tmp_file_path, dataset_name)
            
            return {
                "success": result["success"],
                "dataset_name": dataset_name,
                "processing_result": result
            }
            
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            Path(tmp_file_path).unlink(missing_ok=True)
            
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/datasets/{dataset_name}")
async def get_dataset_info(dataset_name: str):
    """íŠ¹ì • ë°ì´í„°ì…‹ì˜ ìƒì„¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        dataset_info = dataset_manager.get_dataset_info(dataset_name)
        
        if dataset_info is None:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_name}")
        
        return dataset_info
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 

# ì´ë¯¸ì§€ ì¡°íšŒ API
@app.get("/api/image/{project_id}/{dataset_id}/{filename}")
async def get_image(project_id: str, dataset_id: str, filename: str):
    try:
        file_path = BASE_DATA_PATH / project_id / dataset_id / filename
        
        if not file_path.exists():
            raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ì´ë¯¸ì§€ íŒŒì¼ì„ ì§ì ‘ StreamingResponseë¡œ ì „ì†¡
        return FileResponse(str(file_path))

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì´ë¯¸ì§€ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

# =============================================================================
# í•™ìŠµ ê´€ë¦¬ API
# =============================================================================

@app.get("/training/status")
async def get_training_status():
    """í˜„ì¬ í•™ìŠµ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        status = trainer.get_training_status()
        
        return {
            "is_training_active": trainer.is_training_active(),
            "current_training": status
        }
        
    except Exception as e:
        logger.error(f"í•™ìŠµ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/training/start")
async def start_training(config: TrainingConfigModel):
    """ë°ì´í„°ì…‹ íƒ€ì…ë³„ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤"""
    try:
        # ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸
        dataset_info = dataset_manager.get_dataset_info(config.dataset_name)
        if dataset_info is None:
            raise HTTPException(status_code=404, detail=f"ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config.dataset_name}")
        
        # ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì¸ì§€ í™•ì¸
        if trainer.is_training_active():
            raise HTTPException(status_code=400, detail="ì´ë¯¸ í•™ìŠµì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
        
        # ë°ì´í„°ì…‹ íƒ€ì…ì— ë”°ë¥¸ í•™ìŠµ ë¶„ê¸°
        raw_type = dataset_info.get("type", "anomaly")
        dataset_type = "anomaly" if raw_type == "anomal" else raw_type
                
        logger.info(
            f"Start training: ds={config.dataset_name} raw_type={raw_type} -> use={dataset_type} "
            f"lr={config.learning_rate} bs={config.batch_size} epochs={config.epochs} "
            f"split=({config.train_ratio}/{config.val_ratio}/{config.test_ratio}) "
            f"training_type={config.training_type}"
        )
        
        if dataset_type == "anomaly":  # Anomaly Detection
            # PatchCore ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
            patchcore_config = PatchCoreConfig(
                backbone=config.backbone,
                layers=config.layers,
                anomaly_scorer_num_nn=config.anomaly_scorer_num_nn,
                sampling_ratio=config.sampling_ratio,
                dataset_name=config.dataset_name,
                resize_size=config.resize_size,
                image_size=config.image_size,
                gpu_id=config.gpu_id
            )
            
            logger.info(f"Anomaly Detection í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            result = trainer.start_training(
                config=patchcore_config,
                dataset_path=dataset_info["path"],
                blocking=False
            )
            
        elif dataset_type == "classify":  # Classification
            logger.info(f"Classification í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Classification í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            return {"success": False, "error": "Classification ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        elif dataset_type == "obd":  # Object Detection
            logger.info(f"Object Detection í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Object Detection í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)  
            return {"success": False, "error": "Object Detection ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        elif dataset_type == "seg":  # Segmentation
            logger.info(f"Segmentation í•™ìŠµ ì‹œì‘: {config.dataset_name}")
            # Segmentation í•™ìŠµ ë¡œì§ (ì¶”í›„ êµ¬í˜„)
            return {"success": False, "error": "Segmentation ëª¨ë¸ì€ ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"}
            
        else:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì…‹ íƒ€ì…: {dataset_type}")
        
        if result["success"]:
            return {
                "success": True,
                "message": f"{dataset_type} ëª¨ë¸ í•™ìŠµì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
                "training_info": result["training_info"],
                "dataset_type": dataset_type
            }
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨"))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í•™ìŠµ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/training/stop")
async def stop_training():
    """ì§„í–‰ ì¤‘ì¸ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤."""
    try:
        if not trainer.is_training_active():
            raise HTTPException(status_code=400, detail="ì§„í–‰ ì¤‘ì¸ í•™ìŠµì´ ì—†ìŠµë‹ˆë‹¤")
        
        success = trainer.stop_training()
        
        if success:
            return {"success": True, "message": "í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤"}
        else:
            raise HTTPException(status_code=500, detail="í•™ìŠµ ì¤‘ë‹¨ ì‹¤íŒ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í•™ìŠµ ì¤‘ë‹¨ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
# í•™ìŠµ ì§„í–‰ë¥  API ì¶”ê°€
@app.get("/training/progress")
async def get_training_progress():
    """ì‹¤ì‹œê°„ í•™ìŠµ ì§„í–‰ë¥  ì¡°íšŒ"""
    try:
        status = trainer.get_training_status()
        progress = 0
        
        if status and status.get("log_group"):
            progress = parse_training_log(status["log_group"])
            
        return {
            "success": True,
            "status": status,
            "progress": progress,
            "is_active": trainer.is_training_active()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# í•™ìŠµ ë¡œê·¸ API ì¶”ê°€
@app.get("/training/logs/{log_group}")
async def get_training_logs(log_group: str, lines: int = 50):
    """í•™ìŠµ ë¡œê·¸ ì¡°íšŒ"""
    try:
        log_file = trainer.results_dir / f"{log_group}_training.log"
        if not log_file.exists():
            raise HTTPException(status_code=404, detail="ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        with open(log_file, 'r') as f:
            all_lines = f.readlines()
            recent_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
        
        return {
            "success": True,
            "logs": recent_lines,
            "total_lines": len(all_lines)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def parse_training_log(log_group: str) -> int:
    """ë¡œê·¸ íŒŒì¼ì—ì„œ ì§„í–‰ë¥  ì¶”ì¶œ"""
    try:
        log_file = trainer.results_dir / f"{log_group}_training.log"
        if not log_file.exists():
            return 0
            
        with open(log_file, 'r') as f:
            content = f.read()
            
        # PatchCore ë¡œê·¸ì—ì„œ ì§„í–‰ë¥  íŒŒì‹±
        import re
        
        # "Epoch 3/10" íŒ¨í„´ ì°¾ê¸°
        epoch_pattern = r'Epoch (\d+)/(\d+)'
        matches = re.findall(epoch_pattern, content)
        
        if matches:
            current, total = map(int, matches[-1])
            return min(int((current / total) * 100), 100)
            
        # "Training progress: 45%" ê°™ì€ íŒ¨í„´ë„ ì°¾ê¸°
        progress_pattern = r'progress:\s*(\d+)%'
        progress_matches = re.findall(progress_pattern, content, re.IGNORECASE)
        
        if progress_matches:
            return min(int(progress_matches[-1]), 100)
        
        return 0
    except Exception:
        return 0

# =============================================================================
# ëª¨ë¸ ê´€ë¦¬ API
# =============================================================================

@app.get("/models")
async def list_models():
    try:
        trained_models = model_manager.discover_trained_models()
        uploaded_models = []
        saved_models_dir = Path("saved_models")

        if saved_models_dir.exists():
            for model_dir in saved_models_dir.iterdir():
                if model_dir.is_dir():
                    model_info_file = model_dir / "model_info.json"
                    if model_info_file.exists():
                        with open(model_info_file, "r", encoding="utf-8") as f:
                            model_info = json.load(f)
                        uploaded_models.append(model_info)

        all_models = trained_models + uploaded_models

        # âœ… camelCase ë³€í™˜
        all_models_camel = []
        for m in all_models:
            all_models_camel.append(to_camel_case(m))

        return {
            "models": all_models_camel,
            "totalCount": len(all_models_camel)
        }
    except Exception as e:
        logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/models/{model_name}/archive")
async def create_model_archive(model_name: str):
    """ëª¨ë¸ì„ ì••ì¶• ì•„ì¹´ì´ë¸Œë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë¸ ì°¾ê¸°
        models = model_manager.discover_trained_models()
        target_model = None
        
        for model in models:
            if model["model_name"] == model_name:
                target_model = model
                break
        
        if target_model is None:
            raise HTTPException(status_code=404, detail=f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_name}")
        
        # ì•„ì¹´ì´ë¸Œ ìƒì„±
        result = model_manager.create_model_archive(target_model)
        
        if result["success"]:
            return result
        else:
            raise HTTPException(status_code=500, detail=result.get("error", "ì•„ì¹´ì´ë¸Œ ìƒì„± ì‹¤íŒ¨"))
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì•„ì¹´ì´ë¸Œ ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/models/best")
async def get_best_model(metric: str = "image_auroc"):
    """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        models = model_manager.discover_trained_models()
        
        if not models:
            raise HTTPException(status_code=404, detail="í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
        
        best_model = model_manager.get_best_model(models, metric)
        
        if best_model is None:
            raise HTTPException(status_code=404, detail=f"{metric} ê¸°ì¤€ìœ¼ë¡œ ìµœê³  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return {
            "best_model": best_model,
            "metric": metric,
            "score": best_model["metadata"]["performance"].get(metric, None)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìµœê³  ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    
@app.post("/models/upload")
async def upload_model(
    file: UploadFile = File(...),
    project_id: Optional[str] = Form(None)
):
    """ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„"""
    try:
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        file_ext = file.filename.split('.')[-1].lower()
        if file_ext not in ['pkl', 'pth']:
            raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤")
        
        # ëª¨ë¸ ID ìƒì„±
        model_id = f"model_{int(datetime.now().timestamp())}"
        model_name = file.filename.rsplit('.', 1)[0]
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        saved_models_dir = Path("saved_models")
        saved_models_dir.mkdir(exist_ok=True)
        
        model_dir = saved_models_dir / f"{model_name}_{model_id}"
        model_dir.mkdir(parents=True, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        model_path = model_dir / file.filename
        with open(model_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # pkl íŒŒì¼ ë¶„ì„
        model_info = analyze_uploaded_model(str(model_path), model_id, model_name, project_id)
        
        # model_info.json ì €ì¥
        with open(model_dir / "model_info.json", 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: {model_path}")
        return {"success": True, "model": model_info}
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")

@app.delete("/models/{model_id}")
async def delete_model(model_id: str):
    """ì—…ë¡œë“œëœ ëª¨ë¸ ì‚­ì œ"""
    try:
        saved_models_dir = Path("saved_models")
        
        if not saved_models_dir.exists():
            raise HTTPException(status_code=404, detail="ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        for model_dir in saved_models_dir.iterdir():
            if model_dir.is_dir():
                model_info_file = model_dir / "model_info.json"
                
                if model_info_file.exists():
                    try:
                        with open(model_info_file, 'r', encoding='utf-8') as f:
                            model_info = json.load(f)
                        
                        if model_info.get('id') == model_id:
                            shutil.rmtree(model_dir)
                            logger.info(f"ëª¨ë¸ ì‚­ì œ ì™„ë£Œ: {model_dir}")
                            return {"success": True, "message": "ëª¨ë¸ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}
                    except Exception:
                        continue
        
        raise HTTPException(status_code=404, detail="ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ëª¨ë¸ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì‚­ì œ ì‹¤íŒ¨: {str(e)}")

def analyze_uploaded_model(file_path: str, model_id: str, model_name: str, project_id: str = None):
    """ì—…ë¡œë“œëœ pkl íŒŒì¼ì—ì„œ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ"""
    analysis_status = "success"
    error_message = None
    
    try:
        with open(file_path, 'rb') as f:
            model_data = pickle.load(f)

        logger.info(f"=== ëª¨ë¸ íŒŒì¼ ë¶„ì„ ì‹œì‘: {model_name} ===")
        logger.info(f"ë°ì´í„° íƒ€ì…: {type(model_data)}")
        
        if isinstance(model_data, dict):
            logger.info(f"ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(model_data.keys())}")
            for key, value in model_data.items():
                logger.info(f"  {key}: {type(value)}")
        else:
            logger.info(f"ê°ì²´ ì†ì„±ë“¤: {dir(model_data)}")
        
        logger.info("=== ë¶„ì„ ì™„ë£Œ ===")
        
        
        # ê¸°ë³¸ê°’ (ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
        performance = {
            "accuracy": None,
            "f1_score": None,
            "precision": None,
            "recall": None
        }
        
        config = {
            "backbone": None,
            "dataset_name": None,
            "epochs": None,
            "learning_method": None
        }
        
        # ì‹¤ì œ pkl ë°ì´í„° êµ¬ì¡° ë¶„ì„
        logger.info(f"ë¶„ì„ ì¤‘ì¸ ëª¨ë¸ ë°ì´í„° íƒ€ì…: {type(model_data)}")
        
        if isinstance(model_data, dict):
            logger.info(f"ëª¨ë¸ ë°ì´í„° í‚¤ë“¤: {list(model_data.keys())}")
            
            # ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì‹œë„
            if 'performance' in model_data:
                perf_data = model_data['performance']
                if isinstance(perf_data, dict):
                    performance.update(perf_data)
                    logger.info("ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            elif 'metrics' in model_data:
                metrics_data = model_data['metrics']
                if isinstance(metrics_data, dict):
                    performance.update(metrics_data)
                    logger.info("ë©”íŠ¸ë¦­ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            elif 'test_results' in model_data:
                test_data = model_data['test_results']
                if isinstance(test_data, dict):
                    # test_resultsì—ì„œ ì„±ëŠ¥ ì§€í‘œ ë§¤í•‘
                    if 'image_auroc' in test_data:
                        performance['accuracy'] = test_data['image_auroc']
                    if 'pixel_auroc' in test_data:
                        performance['f1_score'] = test_data['pixel_auroc']
                    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            # ì„¤ì • ë°ì´í„° ì¶”ì¶œ ì‹œë„
            if 'config' in model_data:
                config_data = model_data['config']
                if isinstance(config_data, dict):
                    config.update(config_data)
                    logger.info("ì„¤ì • ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ")
            
            # ë°±ë³¸ ì •ë³´ ì¶”ì¶œ
            if 'backbone' in model_data:
                config['backbone'] = model_data['backbone']
            elif 'model' in model_data and hasattr(model_data['model'], '__class__'):
                config['backbone'] = model_data['model'].__class__.__name__
            
            # PatchCore íŠ¹í™” ë¶„ì„
            if 'anomaly_maps' in model_data or 'patch_lib' in model_data:
                config['learning_method'] = 'PatchCore'
                logger.info("PatchCore ëª¨ë¸ë¡œ ì‹ë³„ë¨")
        
        else:
            # dictê°€ ì•„ë‹Œ ê²½ìš° (ëª¨ë¸ ê°ì²´ ë“±)
            logger.info(f"ëª¨ë¸ì´ dictê°€ ì•„ë‹˜. ê°ì²´ ë¶„ì„ ì‹œë„: {model_data.__class__.__name__ if hasattr(model_data, '__class__') else 'unknown'}")
            config['backbone'] = getattr(model_data, '__class__', {}).get('__name__', 'unknown')
            analysis_status = "partial"
            error_message = "ëª¨ë¸ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦„"
        
        # ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        all_performance_none = all(v is None for v in performance.values())
        all_config_none = all(v is None for v in config.values())
        
        if all_performance_none and all_config_none:
            analysis_status = "failed"
            error_message = "ëª¨ë¸ì—ì„œ ì„±ëŠ¥ ë°ì´í„°ë‚˜ ì„¤ì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
        
        return {
            "id": model_id,
            "model_name": model_name,
            "project_id": project_id,
            "config": config,
            "performance": performance,
            "training_info": {
                "training_time": "ì—…ë¡œë“œë¨",
                "image_count": 0,
                "created_date": datetime.now().isoformat(),
                "file_size": Path(file_path).stat().st_size
            },
            "model_path": file_path,
            "status": "uploaded",
            "analysis_status": analysis_status,
            "analysis_error": error_message
        }
        
    except Exception as e:
        error_message = str(e)
        logger.error(f"ëª¨ë¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "id": model_id,
            "model_name": model_name,
            "project_id": project_id,
            "config": {
                "backbone": "ë¶„ì„ ì‹¤íŒ¨",
                "dataset_name": "ì—…ë¡œë“œë¨",
                "epochs": 0,
                "learning_method": "ë¶„ì„ ì‹¤íŒ¨"
            },
            "performance": {
                "accuracy": 0.0,
                "f1_score": 0.0,
                "precision": 0.0,
                "recall": 0.0
            },
            "training_info": {
                "training_time": "ì—…ë¡œë“œë¨",
                "image_count": 0,
                "created_date": datetime.now().isoformat()
            },
            "status": "analysis_failed",
            "analysis_status": "failed",
            "analysis_error": error_message
        }

# =============================================================================
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    import subprocess
    import sys
    
    logger.info("ğŸš€ MLOps Vision Platform ì‹œì‘ ì¤‘...")
    logger.info("ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8080 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”")
    
    try:
        # uvicornì„ subprocessë¡œ ì‹¤í–‰ (ê°€ì¥ ì•ˆì •ì ì¸ ë°©ë²•)
        cmd = [
            sys.executable, "-m", "uvicorn", 
            "mlops_api:app", 
            "--host", "127.0.0.1", 
            "--port", "8080", 
            "--reload"
        ]
        
        logger.info("ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        subprocess.run(cmd)
        
    except KeyboardInterrupt:
        logger.info("ì„œë²„ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        logger.info("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        logger.info("uvicorn mlops_api:app --host 127.0.0.1 --port 8080 --reload")